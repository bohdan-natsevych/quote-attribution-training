{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BookNLP Maximum Performance Quote Attribution (Unified)\n",
        "\n",
        "**Goal**: Train the max-performance quote attribution model (80â€“90% accuracy) in either Kaggle or Colab via one RUN_ENV-aware notebook.\n",
        "\n",
        "**Features**\n",
        "- DeBERTa-v3-large with quote/candidate masks + [QUOTE], [ALTQUOTE], [PAR]\n",
        "- Candidate-level softmax with label smoothing; optional R-Drop; optional temperature scaling\n",
        "- Optional multi-source loading + genre-balanced sampler; PDNC fallback; configurable hard negatives\n",
        "- Curriculum sampler + light augmentation; gradient checkpointing + FP16\n",
        "- Auto checkpoint/resume (model/optimizer/scheduler/best_acc) with cadence set by RUN_ENV; bucketed eval + placeholder postprocess hook\n",
        "\n",
        "**Requirements**\n",
        "- Kaggle: T4 x2 accelerator; storage under `/kaggle/working`\n",
        "- Colab: T4 GPU; storage in Drive at `/content/drive`\n",
        "\n",
        "**Quick start**\n",
        "1) Set `RUN_ENV = \"kaggle\"` or `\"colab\"` in the next cell (default: kaggle).\n",
        "2) Kaggle: no Drive mount; repo/output in `/kaggle/working`; multi-GPU via `accelerate`; checkpoints/evals every 500 steps; auto-resume from latest `checkpoint_*.pt`.\n",
        "3) Colab: mounts Drive to `/content/drive`; repo in `/content`; outputs in Drive; single-GPU (no DDP) with gradient accumulation; checkpoints/evals every 300 steps; auto-resume from latest `checkpoint_*.pt`.\n",
        "4) Run all cellsâ€”data is cloned automatically from the repo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RUN_ENV toggle\n",
        "Set `RUN_ENV = \"kaggle\"` or `\"colab\"` in the next cell (default: kaggle). Paths, checkpoint cadence, and mounts adjust automatically. Kaggle uses multi-GPU via `accelerate` (no Drive mount); Colab mounts Drive, runs single-GPU with gradient accumulation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, torch\n",
        "\n",
        "# CURSOR: Toggle once; everything else keys off this value\n",
        "RUN_ENV = os.environ.get(\"RUN_ENV\", \"kaggle\").strip().lower()\n",
        "ENV_CFG = {\n",
        "    \"kaggle\": {\n",
        "        \"base_dir\": \"/kaggle/working\",\n",
        "        \"repo_dir\": \"/kaggle/working/speaker-attribution-acl2023\",\n",
        "        \"output_root\": \"/kaggle/working\",\n",
        "        \"checkpoint_every\": 500,\n",
        "        \"eval_every\": 500,\n",
        "        \"grad_accum\": 4,\n",
        "        \"use_accelerate\": True,\n",
        "        \"mount_drive\": False,\n",
        "    },\n",
        "    \"colab\": {\n",
        "        \"base_dir\": \"/content/drive/MyDrive/quote_attribution\",\n",
        "        \"repo_dir\": \"/content/speaker-attribution-acl2023\",\n",
        "        \"output_root\": \"/content/drive/MyDrive/quote_attribution\",\n",
        "        \"checkpoint_every\": 300,\n",
        "        \"eval_every\": 300,\n",
        "        \"grad_accum\": 16,\n",
        "        \"use_accelerate\": False,  # single-GPU path\n",
        "        \"mount_drive\": True,\n",
        "    },\n",
        "}\n",
        "assert RUN_ENV in ENV_CFG, f\"Unsupported RUN_ENV: {RUN_ENV}\"\n",
        "ENV = ENV_CFG[RUN_ENV]\n",
        "\n",
        "if ENV[\"mount_drive\"]:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "    os.makedirs(ENV[\"base_dir\"], exist_ok=True)\n",
        "\n",
        "print(f\"Python: {sys.version}\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPUs: {torch.cuda.device_count()}\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "else:\n",
        "    raise RuntimeError(\"GPU not available; enable a GPU runtime.\")\n",
        "\n",
        "REPO_DIR = ENV[\"repo_dir\"]\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    print(\"\\nðŸ“¥ Cloning repository with data...\")\n",
        "    !git clone https://github.com/Priya22/speaker-attribution-acl2023.git {REPO_DIR}\n",
        "else:\n",
        "    print(f\"âœ… Repository present at {REPO_DIR}\")\n",
        "\n",
        "DATA_DIR = f\"{REPO_DIR}/training/data/pdnc\"\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    raise FileNotFoundError(f\"PDNC data missing at {DATA_DIR}\")\n",
        "else:\n",
        "    print(f\"âœ… PDNC data found at {DATA_DIR}\")\n",
        "\n",
        "BASE_DIR = ENV[\"base_dir\"]\n",
        "OUTPUT_ROOT = ENV[\"output_root\"]\n",
        "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
        "print(f\"Output root: {OUTPUT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q transformers>=4.30.0 accelerate>=0.20.0 datasets scikit-learn tqdm nlpaug nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "TARGET_LEVEL = 1  # 1=PDNC, 2=multi-source, 3=ensemble placeholder\n",
        "\n",
        "# CURSOR: For best generalization on unknown/new books, train all 5 folds\n",
        "# Set to \"all\" for all folds, or list of fold indices [0, 1, 2, 3, 4] or [0, 2] etc.\n",
        "FOLD_SELECTION = \"all\"  # \"all\" or list like [0, 1, 2] or [3]\n",
        "SPLIT_TYPE = \"leave-x-out\"  # \"leave-x-out\" or \"random\"\n",
        "\n",
        "CONFIGS = {\n",
        "    1: {\n",
        "        'name': 'Target 1: DeBERTa-large + Augmentation',\n",
        "        'epochs': 50, 'batch_size': 8, 'lr': 5e-6,\n",
        "        'use_augmentation': True, 'use_curriculum': True,\n",
        "        'focal_gamma': 2.0, 'label_smoothing': 0.1, 'r_drop_alpha': 0.7,\n",
        "        'target_accuracy': 0.85,\n",
        "        'hard_negative_topk': 2,\n",
        "        'calibrate_temperature': True,\n",
        "        'use_multi_source': False,\n",
        "        'use_postprocess': False,\n",
        "        'balance_genres': False,\n",
        "        'fold_selection': FOLD_SELECTION,\n",
        "        'split_type': SPLIT_TYPE,\n",
        "    },\n",
        "    2: {\n",
        "        'name': 'Target 2: Multi-Source + Genre Balancing',\n",
        "        'epochs': 30, 'batch_size': 8, 'lr': 2e-6,\n",
        "        'use_augmentation': True, 'use_curriculum': True,\n",
        "        'balance_genres': True, 'min_genre_acc': 0.75,\n",
        "        'target_accuracy': 0.88,\n",
        "        'hard_negative_topk': 2,\n",
        "        'calibrate_temperature': True,\n",
        "        'use_multi_source': True,\n",
        "        'use_postprocess': False,\n",
        "        'fold_selection': FOLD_SELECTION,\n",
        "        'split_type': SPLIT_TYPE,\n",
        "    },\n",
        "    3: {\n",
        "        'name': 'Target 3: Ensemble + Distillation',\n",
        "        'ensemble_models': ['microsoft/deberta-v3-large', 'roberta-large'],\n",
        "        'student_model': 'microsoft/deberta-v3-base',\n",
        "        'epochs': 10, 'batch_size': 4, 'lr': 5e-6,\n",
        "        'distill_epochs': 10, 'temperature': 3.0, 'alpha': 0.7,\n",
        "        'target_accuracy': 0.90,\n",
        "        'use_multi_source': True,\n",
        "        'use_augmentation': True,\n",
        "        'use_curriculum': True,\n",
        "        'balance_genres': True,\n",
        "        'hard_negative_topk': 2,\n",
        "        'calibrate_temperature': True,\n",
        "        'use_postprocess': False,\n",
        "        'fold_selection': FOLD_SELECTION,\n",
        "        'split_type': SPLIT_TYPE,\n",
        "    }\n",
        "}\n",
        "\n",
        "CONFIG = CONFIGS[TARGET_LEVEL].copy()\n",
        "CONFIG.update({\n",
        "    'base_model': 'microsoft/deberta-v3-large',\n",
        "    'max_length': 512,\n",
        "    'gradient_accumulation_steps': ENV['grad_accum'],\n",
        "    'checkpoint_every': ENV['checkpoint_every'],  # CURSOR: env-specific cadence\n",
        "    'eval_every': ENV['eval_every'],\n",
        "    'fp16': True,\n",
        "    'gradient_checkpointing': True,\n",
        "    'seed': 42,\n",
        "    'data_path': f'{REPO_DIR}/data/pdnc_source',\n",
        "    'train_data_path': f'{REPO_DIR}/training/data/pdnc',\n",
        "    'output_dir': f'{OUTPUT_ROOT}/target_{TARGET_LEVEL}',\n",
        "    'multi_source_base': f'{REPO_DIR}/data',\n",
        "    'use_accelerate': ENV['use_accelerate'],\n",
        "    # CURSOR: Feature toggles (disabled by default)\n",
        "    'use_combined_loss': True,\n",
        "    'use_postprocess': True,\n",
        "    'postprocess_confidence': 0.6,\n",
        "    'use_ensemble_eval': True,\n",
        "    'ensemble_model_names': ['microsoft/deberta-v3-large'],\n",
        "    'ensemble_voting_strategy': 'weighted_average',\n",
        "    'run_cross_domain_validation': True,\n",
        "    'run_genre_adaptation': True,\n",
        "    'run_error_analysis': True,\n",
        "    'run_model_optimization': True,\n",
        "    'optimize_quantize': True,\n",
        "    'optimize_export_onnx': True,\n",
        "})\n",
        "\n",
        "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
        "print(f\"Selected: {CONFIG['name']}\")\n",
        "print(f\"Target accuracy: {CONFIG['target_accuracy']:.0%}\")\n",
        "print(f\"Output dir: {CONFIG['output_dir']}\")\n",
        "print(f\"RUN_ENV: {RUN_ENV} | checkpoint_every={CONFIG['checkpoint_every']} | grad_accum={CONFIG['gradient_accumulation_steps']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CURSOR: Auto-download multi-source datasets when enabled\n",
        "import shutil\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "if CONFIG.get(\"use_multi_source\", False):\n",
        "    multi_base = Path(CONFIG[\"multi_source_base\"])\n",
        "    multi_base.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def clone_if_missing(url: str, dest: Path) -> bool:\n",
        "        if dest.exists() and any(dest.iterdir()):\n",
        "            print(f\"âœ… Found {dest}\")\n",
        "            return False\n",
        "        print(f\"ðŸ“¥ Cloning {url} -> {dest}\")\n",
        "        subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", url, str(dest)], check=True)\n",
        "        return True\n",
        "\n",
        "    lit_dir = multi_base / \"litbank\"\n",
        "    clone_if_missing(\"https://github.com/dbamman/litbank.git\", lit_dir)\n",
        "\n",
        "    dq_dir = multi_base / \"directquote\"\n",
        "    if not dq_dir.exists() or not any(dq_dir.iterdir()):\n",
        "        tmp_repo = multi_base / \"directquote_repo_tmp\"\n",
        "        if tmp_repo.exists():\n",
        "            shutil.rmtree(tmp_repo, ignore_errors=True)\n",
        "        clone_if_missing(\"https://github.com/THUNLP-MT/DirectQuote.git\", tmp_repo)\n",
        "\n",
        "        dq_dir.mkdir(parents=True, exist_ok=True)\n",
        "        data_src = tmp_repo / \"data\"\n",
        "        copied = 0\n",
        "\n",
        "        if data_src.exists():\n",
        "            for src in data_src.glob(\"**/*.json*\"):\n",
        "                if src.is_file():\n",
        "                    target = dq_dir / src.name\n",
        "                    shutil.copy2(src, target)\n",
        "                    copied += 1\n",
        "\n",
        "        if copied == 0:\n",
        "            for src in tmp_repo.glob(\"*.json*\"):\n",
        "                if src.is_file():\n",
        "                    target = dq_dir / src.name\n",
        "                    shutil.copy2(src, target)\n",
        "                    copied += 1\n",
        "\n",
        "        shutil.rmtree(tmp_repo, ignore_errors=True)\n",
        "        print(f\"âœ… DirectQuote files copied: {copied}\")\n",
        "    else:\n",
        "        print(f\"âœ… Found {dq_dir}\")\n",
        "else:\n",
        "    print(\"Multi-source disabled; skipping extra dataset downloads.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob, random, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "from transformers import DebertaV2Model, DebertaV2Tokenizer\n",
        "from accelerate import Accelerator\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from training.data.multi_source_data import MultiSourceDataLoader\n",
        "from training.data.data_augmentation import QuoteAugmenter\n",
        "from training.data.curriculum_loader import DifficultyClassifier, CurriculumSampler, CurriculumConfig\n",
        "from training.evaluation.confidence_calibration import TemperatureScaling\n",
        "from training.models.max_performance_model import MaxPerformanceSpeakerModel\n",
        "from training.losses.focal_loss import CombinedLoss\n",
        "from training.optimization.post_processing import PostProcessor\n",
        "from training.evaluation.cross_domain_validation import CrossDomainValidator\n",
        "from training.evaluation.genre_specific_adaptations import GenreSpecificAdaptation\n",
        "from training.evaluation.error_analysis import ErrorAnalyzer\n",
        "from training.models.ensemble import create_ensemble\n",
        "from training.optimization.model_optimization import optimize_for_inference\n",
        "\n",
        "# CURSOR: Deterministic setup for reproducibility\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(CONFIG['seed'])\n",
        "\n",
        "accelerator = None\n",
        "if CONFIG['use_accelerate']:\n",
        "    accelerator = Accelerator(\n",
        "        mixed_precision='fp16' if CONFIG['fp16'] else 'no',\n",
        "        gradient_accumulation_steps=CONFIG['gradient_accumulation_steps']\n",
        "    )\n",
        "    device = accelerator.device\n",
        "else:\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"Device: {device} | RUN_ENV={RUN_ENV} | accelerate={bool(accelerator)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the richer model with span/mask support\n",
        "\n",
        "class QuoteDataset(Dataset):\n",
        "    def __init__(self, samples, tokenizer, max_length=512, augment=False, augmenter: QuoteAugmenter = None):\n",
        "        self.samples, self.tok, self.max_len = samples, tokenizer, max_length\n",
        "        self.augment = augment\n",
        "        self.augmenter = augmenter\n",
        "        self.par_id = self.tok.convert_tokens_to_ids(\"[PAR]\")\n",
        "        self.altq_id = self.tok.convert_tokens_to_ids(\"[ALTQUOTE]\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def _maybe_augment_text(self, text: str) -> str:\n",
        "        if not self.augment or not self.augmenter:\n",
        "            return text\n",
        "        # CURSOR: Light, safe synonym swap\n",
        "        if random.random() < 0.2:\n",
        "            try:\n",
        "                return self.augmenter.synonym_replace(text, protected_spans=[], n=2)\n",
        "            except Exception:\n",
        "                return text\n",
        "        return text\n",
        "\n",
        "    def _encode(self, sample):\n",
        "        base_text = self._maybe_augment_text(sample['text'])\n",
        "        base_ids = self.tok.encode(base_text, add_special_tokens=False)\n",
        "        candidates = sample['candidates']\n",
        "        cand_ids = [self.tok.encode(c, add_special_tokens=False) for c in candidates]\n",
        "\n",
        "        reserved = 1 + 1 + sum(1 + len(ci) for ci in cand_ids)\n",
        "        room = max(self.max_len - reserved, 8)\n",
        "        if len(base_ids) > room:\n",
        "            base_ids = base_ids[:room]\n",
        "\n",
        "        tokens = [self.par_id] + base_ids + [self.altq_id]\n",
        "        quote_mask = [1] * len(tokens)\n",
        "\n",
        "        cand_masks = []\n",
        "        for ci in cand_ids:\n",
        "            tokens.append(self.par_id)\n",
        "            start = len(tokens)\n",
        "            tokens.extend(ci)\n",
        "            end = len(tokens)\n",
        "            mask = [0] * len(tokens)\n",
        "            for i in range(start, end):\n",
        "                mask[i] = 1\n",
        "            cand_masks.append(mask)\n",
        "\n",
        "        if not cand_masks:\n",
        "            tokens.append(self.par_id)\n",
        "            mask = [0] * len(tokens)\n",
        "            cand_masks.append(mask)\n",
        "\n",
        "        tokens = tokens[: self.max_len]\n",
        "        attention = [1] * len(tokens)\n",
        "        if len(tokens) < self.max_len:\n",
        "            pad_len = self.max_len - len(tokens)\n",
        "            tokens += [self.tok.pad_token_id] * pad_len\n",
        "            attention += [0] * pad_len\n",
        "            quote_mask += [0] * pad_len\n",
        "            cand_masks = [cm + [0] * pad_len for cm in cand_masks]\n",
        "        else:\n",
        "            quote_mask = quote_mask[: self.max_len]\n",
        "            cand_masks = [cm[: self.max_len] for cm in cand_masks]\n",
        "\n",
        "        return tokens, attention, quote_mask, cand_masks\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        tokens, attention, quote_mask, cand_masks = self._encode(sample)\n",
        "        label_idx = sample['gold_index'] if sample['gold_index'] >= 0 else -100\n",
        "        return {\n",
        "            'input_ids': torch.tensor(tokens, dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(attention, dtype=torch.long),\n",
        "            'quote_mask': torch.tensor(quote_mask, dtype=torch.long),\n",
        "            'candidate_masks': [torch.tensor(cm, dtype=torch.long) for cm in cand_masks],\n",
        "            'label_idx': torch.tensor(label_idx, dtype=torch.long),\n",
        "            'quote_id': sample['quote_id']\n",
        "        }\n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, label_smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.gamma, self.ls = gamma, label_smoothing\n",
        "    def forward(self, inputs, targets):\n",
        "        smoothed = targets.float() * (1 - self.ls) + 0.5 * self.ls\n",
        "        probs = torch.sigmoid(inputs)\n",
        "        ce = F.binary_cross_entropy(probs, smoothed, reduction='none')\n",
        "        pt = torch.where(targets > 0, probs, 1 - probs).clamp(min=1e-6, max=1-1e-6)\n",
        "        return ((1 - pt) ** self.gamma * ce).mean()\n",
        "\n",
        "\n",
        "# Data helpers\n",
        "import json\n",
        "\n",
        "def _split_candidates(raw: str):\n",
        "    if not raw:\n",
        "        return []\n",
        "    for sep in [\"||\", \"|\", \";\", \",\"]:\n",
        "        if sep in raw:\n",
        "            return [c.strip() for c in raw.split(sep) if c.strip()]\n",
        "    return [raw.strip()] if raw.strip() else []\n",
        "\n",
        "\n",
        "def _add_hard_negatives(samples, topk):\n",
        "    if not topk or not samples:\n",
        "        return samples\n",
        "    freq = {}\n",
        "    for s in samples:\n",
        "        for c in s['candidates']:\n",
        "            freq[c] = freq.get(c, 0) + 1\n",
        "    sorted_cands = [c for c, _ in sorted(freq.items(), key=lambda x: -x[1])]\n",
        "    for s in samples:\n",
        "        existing = set(s['candidates'])\n",
        "        extras = [c for c in sorted_cands if c not in existing and c != s['gold']][:topk]\n",
        "        s['candidates'] = s['candidates'] + extras\n",
        "    return samples\n",
        "\n",
        "\n",
        "def load_quote_data(filename):\n",
        "    samples = []\n",
        "    with open(filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            cols = line.rstrip().split(\"\\t\")\n",
        "            if len(cols) < 6:\n",
        "                continue\n",
        "            doc_id = cols[0].strip()\n",
        "            quote_id = cols[1].strip()\n",
        "            gold_id = cols[2].strip()\n",
        "            candidate_field = cols[4].strip()\n",
        "            base_text = cols[5]\n",
        "            candidates = _split_candidates(candidate_field)\n",
        "            if not candidates or not gold_id:\n",
        "                continue\n",
        "            qid = f\"{doc_id}:{quote_id}\" if doc_id or quote_id else quote_id\n",
        "            samples.append({\n",
        "                'quote_id': qid,\n",
        "                'text': base_text,\n",
        "                'candidates': candidates,\n",
        "                'gold': gold_id,\n",
        "                'genre': 'pdnc',\n",
        "                'source': 'pdnc'\n",
        "            })\n",
        "    samples = _add_hard_negatives(samples, CONFIG.get('hard_negative_topk', 0))\n",
        "    for s in samples:\n",
        "        s['gold_index'] = s['candidates'].index(s['gold']) if s['gold'] in s['candidates'] else -1\n",
        "    return samples\n",
        "\n",
        "\n",
        "def load_multi_source_samples(base_path: str):\n",
        "    loader = MultiSourceDataLoader(base_path=base_path, seed=CONFIG['seed'])\n",
        "    loader.load_all()\n",
        "    all_samples = []\n",
        "    for source, samples in loader.data_by_source.items():\n",
        "        for i, s in enumerate(samples):\n",
        "            gold = s.get('speaker', '')\n",
        "            text = s.get('text') or s.get('quote') or ''\n",
        "            genre = s.get('genre', source)\n",
        "            if not gold or not text:\n",
        "                continue  # CURSOR: skip entries without speakers/text\n",
        "            qid = f\"{source}:{i}\"\n",
        "            all_samples.append({\n",
        "                'quote_id': qid,\n",
        "                'text': text,\n",
        "                'candidates': [gold],\n",
        "                'gold': gold,\n",
        "                'genre': genre,\n",
        "                'source': source\n",
        "            })\n",
        "    all_samples = _add_hard_negatives(all_samples, CONFIG.get('hard_negative_topk', 0))\n",
        "    for s in all_samples:\n",
        "        s['gold_index'] = s['candidates'].index(s['gold']) if s['gold'] in s['candidates'] else -1\n",
        "    random.shuffle(all_samples)\n",
        "    split_val = int(0.1 * len(all_samples))\n",
        "    split_test = int(0.1 * len(all_samples))\n",
        "    val_samples = all_samples[:split_val]\n",
        "    test_samples = all_samples[split_val:split_val + split_test]\n",
        "    train_samples = all_samples[split_val + split_test:]\n",
        "    return train_samples, val_samples, test_samples\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Data (PDNC or multi-source)\n",
        "use_multi = CONFIG.get('use_multi_source', False)\n",
        "\n",
        "# CURSOR: Determine which folds to train\n",
        "fold_selection = CONFIG.get('fold_selection', [0])\n",
        "split_type = CONFIG.get('split_type', 'leave-x-out')\n",
        "\n",
        "if fold_selection == \"all\":\n",
        "    FOLDS_TO_TRAIN = list(range(5))  # CURSOR: Train all 5 folds for best generalization\n",
        "elif isinstance(fold_selection, list):\n",
        "    FOLDS_TO_TRAIN = fold_selection\n",
        "else:\n",
        "    FOLDS_TO_TRAIN = [int(fold_selection)]\n",
        "\n",
        "print(f\"ðŸ“‹ Folds to train: {FOLDS_TO_TRAIN}\")\n",
        "\n",
        "# CURSOR: Helper to reload data for a specific fold\n",
        "def load_fold_data(fold_idx: int):\n",
        "    \"\"\"Load train/val/test data for a specific fold.\"\"\"\n",
        "    train_f = f\"{CONFIG['train_data_path']}/{split_type}/split_{fold_idx}/quotes.train.txt\"\n",
        "    dev_f = f\"{CONFIG['train_data_path']}/{split_type}/split_{fold_idx}/quotes.dev.txt\"\n",
        "    test_f = f\"{CONFIG['train_data_path']}/{split_type}/split_{fold_idx}/quotes.test.txt\"\n",
        "    return load_quote_data(train_f), load_quote_data(dev_f), load_quote_data(test_f)\n",
        "\n",
        "if use_multi:\n",
        "    print(\"ðŸ“‚ Loading multi-source data...\")\n",
        "    train_samples, val_samples, test_samples = load_multi_source_samples(CONFIG['multi_source_base'])\n",
        "    print(f\"   Train quotes: {len(train_samples)}\")\n",
        "    print(f\"   Val quotes: {len(val_samples)}\")\n",
        "    print(f\"   Test quotes: {len(test_samples)}\")\n",
        "    # CURSOR: For multi-source, we train once (no fold iteration)\n",
        "    FOLDS_TO_TRAIN = [0]\n",
        "else:\n",
        "    # CURSOR: Load data for first fold (will reload in loop for each fold)\n",
        "    current_fold = FOLDS_TO_TRAIN[0]\n",
        "    \n",
        "    train_file = f\"{CONFIG['train_data_path']}/{split_type}/split_{current_fold}/quotes.train.txt\"\n",
        "    dev_file = f\"{CONFIG['train_data_path']}/{split_type}/split_{current_fold}/quotes.dev.txt\"\n",
        "    test_file = f\"{CONFIG['train_data_path']}/{split_type}/split_{current_fold}/quotes.test.txt\"\n",
        "\n",
        "    print(f\"ðŸ“‚ Loading pre-processed PDNC data (Fold {current_fold}, {split_type})...\")\n",
        "    print(f\"   Train: {train_file}\")\n",
        "    print(f\"   Dev: {dev_file}\")\n",
        "    print(f\"   Test: {test_file}\")\n",
        "\n",
        "    train_samples = load_quote_data(train_file)\n",
        "    val_samples = load_quote_data(dev_file)\n",
        "    test_samples = load_quote_data(test_file)\n",
        "\n",
        "    print(f\"\\nâœ… Data loaded!\")\n",
        "    print(f\"   Train quotes: {len(train_samples)}\")\n",
        "    print(f\"   Val quotes: {len(val_samples)}\")\n",
        "    print(f\"   Test quotes: {len(test_samples)}\")\n",
        "\n",
        "val_meta_map = {s['quote_id']: s for s in val_samples}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading model...\")\n",
        "model = MaxPerformanceSpeakerModel(CONFIG.get('base_model', 'microsoft/deberta-v3-large'))\n",
        "if CONFIG['gradient_checkpointing']:\n",
        "    model.encoder.gradient_checkpointing_enable()\n",
        "tokenizer = model.get_tokenizer()\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "augmenter = QuoteAugmenter(seed=CONFIG['seed']) if CONFIG.get('use_augmentation', False) else None\n",
        "post_processor = PostProcessor(confidence_threshold=CONFIG.get('postprocess_confidence', 0.6)) if CONFIG.get('use_postprocess', False) else None\n",
        "ensemble_handle = None\n",
        "\n",
        "# CURSOR: Helper function to create dataloaders for a given dataset\n",
        "def create_dataloaders(train_samples, val_samples, tokenizer, augmenter):\n",
        "    if CONFIG.get('use_curriculum', False):\n",
        "        train_samples = sorted(train_samples, key=lambda s: len(s['text']))\n",
        "\n",
        "    train_dataset = QuoteDataset(train_samples, tokenizer, CONFIG['max_length'], augment=CONFIG.get('use_augmentation', False), augmenter=augmenter)\n",
        "    val_dataset = QuoteDataset(val_samples, tokenizer, CONFIG['max_length'], augment=False, augmenter=None)\n",
        "\n",
        "    def make_genre_sampler(samples):\n",
        "        counts = defaultdict(int)\n",
        "        for s in samples:\n",
        "            counts[s.get('genre', 'unknown')] += 1\n",
        "        weights = []\n",
        "        for s in samples:\n",
        "            g = s.get('genre', 'unknown')\n",
        "            weights.append(1.0 / max(counts[g], 1))\n",
        "        return WeightedRandomSampler(weights=weights, num_samples=len(samples), replacement=True)\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        max_cands = max(len(item['candidate_masks']) for item in batch)\n",
        "        input_ids = torch.stack([b['input_ids'] for b in batch])\n",
        "        attention_mask = torch.stack([b['attention_mask'] for b in batch])\n",
        "        quote_mask = torch.stack([b['quote_mask'] for b in batch])\n",
        "        cand_masks = []\n",
        "        cand_attn = []\n",
        "        for b in batch:\n",
        "            masks = b['candidate_masks']\n",
        "            orig_len = len(masks)\n",
        "            if orig_len == 0:\n",
        "                masks = [torch.zeros_like(b['input_ids'])]\n",
        "                orig_len = 1\n",
        "            pad_count = max_cands - orig_len\n",
        "            if pad_count > 0:\n",
        "                pad_mask = torch.zeros_like(masks[0])\n",
        "                masks = masks + [pad_mask] * pad_count\n",
        "            cand_masks.append(torch.stack(masks))\n",
        "            attn_row = [1] * orig_len + [0] * pad_count\n",
        "            cand_attn.append(torch.tensor(attn_row, dtype=torch.long))\n",
        "        cand_masks = torch.stack(cand_masks)\n",
        "        cand_attn = torch.stack(cand_attn)\n",
        "        labels = torch.stack([b['label_idx'] for b in batch])\n",
        "        quote_ids = [b['quote_id'] for b in batch]\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'quote_mask': quote_mask,\n",
        "            'candidate_masks': cand_masks,\n",
        "            'candidate_attention_mask': cand_attn,\n",
        "            'label_idx': labels,\n",
        "            'quote_ids': quote_ids\n",
        "        }\n",
        "\n",
        "    sampler = None\n",
        "    if CONFIG.get('use_curriculum', False):\n",
        "        difficulty = DifficultyClassifier()\n",
        "        difficulty_indices = difficulty.classify_dataset(train_samples)\n",
        "        sampler = CurriculumSampler(\n",
        "            difficulty_indices=difficulty_indices,\n",
        "            config=CurriculumConfig(),\n",
        "            total_epochs=CONFIG['epochs'],\n",
        "            current_epoch=0,\n",
        "            batch_size=CONFIG['batch_size'],\n",
        "            seed=CONFIG['seed']\n",
        "        )\n",
        "    elif CONFIG.get('use_multi_source', False) and CONFIG.get('balance_genres', False):\n",
        "        sampler = make_genre_sampler(train_samples)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=CONFIG['batch_size'],\n",
        "        shuffle=False if sampler else not CONFIG.get('use_curriculum', False),\n",
        "        sampler=sampler,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
        "    \n",
        "    return train_loader, val_loader\n",
        "\n",
        "# Create initial dataloaders\n",
        "train_loader, val_loader = create_dataloaders(train_samples, val_samples, tokenizer, augmenter)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=CONFIG['lr'])\n",
        "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n",
        "use_combined_loss = CONFIG.get('use_combined_loss', False)\n",
        "loss_fn = CombinedLoss(\n",
        "    focal_gamma=CONFIG.get('focal_gamma', 2.0),\n",
        "    label_smoothing=CONFIG.get('label_smoothing', 0.1),\n",
        "    r_drop_alpha=CONFIG.get('r_drop_alpha', 0.0),\n",
        "    use_focal=CONFIG.get('use_focal_loss', True),\n",
        "    use_label_smoothing=CONFIG.get('label_smoothing', 0.0) > 0,\n",
        "    use_r_drop=CONFIG.get('r_drop_alpha', 0.0) > 0\n",
        ") if use_combined_loss else None\n",
        "base_ce_loss = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "if CONFIG.get('use_ensemble_eval', False):\n",
        "    try:\n",
        "        ensemble_handle = create_ensemble(\n",
        "            model_names=CONFIG.get('ensemble_model_names'),\n",
        "            device=str(device) if isinstance(device, torch.device) else device\n",
        "        )\n",
        "        print(f\"Ensemble initialized with {len(CONFIG.get('ensemble_model_names', []))} model(s)\")\n",
        "    except Exception as exc:\n",
        "        ensemble_handle = None\n",
        "        print(f\"Ensemble init failed: {exc}\")\n",
        "\n",
        "if accelerator:\n",
        "    model, optimizer, train_loader, val_loader = accelerator.prepare(model, optimizer, train_loader, val_loader)\n",
        "    device = accelerator.device\n",
        "    scaler = None\n",
        "else:\n",
        "    model = model.to(device)\n",
        "    scaler = GradScaler(enabled=CONFIG['fp16'])\n",
        "\n",
        "if loss_fn:\n",
        "    loss_fn = loss_fn.to(device)\n",
        "base_ce_loss = base_ce_loss.to(device)\n",
        "\n",
        "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
        "print(f\"Dataloaders ready | train batches: {len(train_loader)} | val batches: {len(val_loader)}\")\n",
        "print(\"Ready to train!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training + evaluation utilities\n",
        "\n",
        "def save_checkpoint(step, metrics, best_acc, epoch, fold_idx=None):\n",
        "    suffix = f\"_fold_{fold_idx}\" if fold_idx is not None else \"\"\n",
        "    if accelerator:\n",
        "        accelerator.wait_for_everyone()\n",
        "        unwrapped = accelerator.unwrap_model(model)\n",
        "        accelerator.save({\n",
        "            'step': step,\n",
        "            'epoch': epoch,\n",
        "            'model': unwrapped.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'scheduler': scheduler.state_dict(),\n",
        "            'metrics': metrics,\n",
        "            'best_acc': best_acc,\n",
        "            'fold': fold_idx\n",
        "        }, f\"{CONFIG['output_dir']}/checkpoint_{step}{suffix}.pt\")\n",
        "    else:\n",
        "        torch.save({\n",
        "            'step': step,\n",
        "            'epoch': epoch,\n",
        "            'model': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'scheduler': scheduler.state_dict(),\n",
        "            'metrics': metrics,\n",
        "            'best_acc': best_acc,\n",
        "            'fold': fold_idx\n",
        "        }, f\"{CONFIG['output_dir']}/checkpoint_{step}{suffix}.pt\")\n",
        "    if (not accelerator) or accelerator.is_main_process:\n",
        "        print(f\"Checkpoint saved at step {step}{suffix}\")\n",
        "\n",
        "\n",
        "def load_latest_checkpoint(fold_idx=None):\n",
        "    import re\n",
        "    suffix = f\"_fold_{fold_idx}\" if fold_idx is not None else \"\"\n",
        "    pattern = f'checkpoint_*{suffix}.pt' if fold_idx is not None else 'checkpoint_*.pt'\n",
        "    ckpts = glob.glob(os.path.join(CONFIG['output_dir'], pattern))\n",
        "    if not ckpts:\n",
        "        return 0, 0.0, 0\n",
        "    # CURSOR: Use numeric parsing to find highest step (not lexicographic)\n",
        "    def extract_step(path):\n",
        "        match = re.search(r'checkpoint_(\\d+)(?:_fold_\\d+)?\\.pt$', path)\n",
        "        return int(match.group(1)) if match else -1\n",
        "    latest = max(ckpts, key=extract_step)\n",
        "    state = torch.load(latest, map_location='cpu')\n",
        "    if accelerator:\n",
        "        unwrapped = accelerator.unwrap_model(model)\n",
        "        unwrapped.load_state_dict(state.get('model', {}))\n",
        "    else:\n",
        "        model.load_state_dict(state.get('model', {}))\n",
        "    optimizer.load_state_dict(state.get('optimizer', {}))\n",
        "    if state.get('scheduler'):\n",
        "        scheduler.load_state_dict(state['scheduler'])\n",
        "    start_step = state.get('step', 0)\n",
        "    start_epoch = state.get('epoch', 0)\n",
        "    best_acc = state.get('best_acc', 0.0)\n",
        "    if (not accelerator) or accelerator.is_main_process:\n",
        "        print(f\"Resumed from {latest} (step {start_step}, epoch {start_epoch}, best_acc {best_acc:.4f})\")\n",
        "    return start_step, best_acc, start_epoch\n",
        "\n",
        "\n",
        "def categorize_sample(meta):\n",
        "    text = meta.get('text', '').lower()\n",
        "    pronouns = set(['he','she','they','him','her','them','his','hers','their','theirs','i','me','my','mine','we','us','our','ours'])\n",
        "    words = text.split()\n",
        "    pronoun_density = sum(1 for w in words if w in pronouns) / max(len(words), 1)\n",
        "    if pronoun_density > 0.12:\n",
        "        return 'pronoun_heavy'\n",
        "    if 'said' in text or 'asked' in text or 'replied' in text:\n",
        "        return 'verb_attribution'\n",
        "    return 'other'\n",
        "\n",
        "\n",
        "def apply_postprocess(pred_idx: int, confidence: float, qid: str) -> int:\n",
        "    if not CONFIG.get('use_postprocess', False) or not post_processor:\n",
        "        return pred_idx\n",
        "    meta = val_meta_map.get(qid, {})\n",
        "    candidates = meta.get('candidates', [])\n",
        "    if not candidates or pred_idx is None or pred_idx < 0 or pred_idx >= len(candidates):\n",
        "        return pred_idx\n",
        "    result = post_processor.process(\n",
        "        speaker=candidates[pred_idx],\n",
        "        confidence=float(confidence),\n",
        "        quote=meta.get('text', ''),\n",
        "        context=meta.get('text', ''),\n",
        "        candidates=candidates,\n",
        "        position=0\n",
        "    )\n",
        "    if result.speaker in candidates:\n",
        "        return candidates.index(result.speaker)\n",
        "    return pred_idx\n",
        "\n",
        "\n",
        "def r_drop_penalty(logits_a, logits_b, alpha):\n",
        "    if alpha <= 0:\n",
        "        return torch.tensor(0.0, device=logits_a.device)\n",
        "    pa = torch.log_softmax(logits_a, dim=-1)\n",
        "    pb = torch.log_softmax(logits_b, dim=-1)\n",
        "    return alpha * 0.5 * ((pa.exp() * (pa - pb)).sum(dim=-1).mean() + (pb.exp() * (pb - pa)).sum(dim=-1).mean())\n",
        "\n",
        "\n",
        "def move_to_device(batch, device):\n",
        "    moved = {}\n",
        "    for k, v in batch.items():\n",
        "        if torch.is_tensor(v):\n",
        "            moved[k] = v.to(device)\n",
        "        elif isinstance(v, list) and v and torch.is_tensor(v[0]):\n",
        "            moved[k] = [t.to(device) for t in v]\n",
        "        else:\n",
        "            moved[k] = v\n",
        "    return moved\n",
        "\n",
        "\n",
        "latest_eval_details = None\n",
        "\n",
        "def evaluate(temp: float = 1.0):\n",
        "    global latest_eval_details\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "    buckets = defaultdict(lambda: {'total':0, 'correct':0})\n",
        "    eval_details = {'preds': [], 'labels': [], 'genres': [], 'confidences': [], 'qids': [], 'samples': []}\n",
        "    is_main = (not accelerator) or accelerator.is_main_process\n",
        "    if post_processor:\n",
        "        post_processor.dialogue_tracker.reset()\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            if accelerator:\n",
        "                logits, _ = model(\n",
        "                    batch['input_ids'],\n",
        "                    batch['attention_mask'],\n",
        "                    batch['quote_mask'],\n",
        "                    batch['candidate_masks'],\n",
        "                    batch['candidate_attention_mask']\n",
        "                )\n",
        "                if 'candidate_attention_mask' in batch and batch['candidate_attention_mask'] is not None:\n",
        "                    logits = logits.masked_fill(batch['candidate_attention_mask'] == 0, -1e9)\n",
        "                probs = torch.softmax(logits / temp, dim=-1)\n",
        "                conf_vals, preds = torch.max(probs, dim=-1)\n",
        "                labels = batch['label_idx']\n",
        "                preds_all = accelerator.gather_for_metrics(preds)\n",
        "                labels_all = accelerator.gather_for_metrics(labels)\n",
        "                conf_all = accelerator.gather_for_metrics(conf_vals)\n",
        "                qids_all = accelerator.gather_object(batch['quote_ids'])\n",
        "                flat_qids = [qid for sub in qids_all for qid in sub]\n",
        "                preds_list = preds_all.cpu().tolist()\n",
        "                labels_list = labels_all.cpu().tolist()\n",
        "                conf_list = conf_all.cpu().tolist()\n",
        "            else:\n",
        "                batch = move_to_device(batch, device)\n",
        "                logits, _ = model(\n",
        "                    batch['input_ids'],\n",
        "                    batch['attention_mask'],\n",
        "                    batch['quote_mask'],\n",
        "                    batch['candidate_masks'],\n",
        "                    batch['candidate_attention_mask']\n",
        "                )\n",
        "                if 'candidate_attention_mask' in batch and batch['candidate_attention_mask'] is not None:\n",
        "                    logits = logits.masked_fill(batch['candidate_attention_mask'] == 0, -1e9)\n",
        "                probs = torch.softmax(logits / temp, dim=-1)\n",
        "                conf_vals, preds = torch.max(probs, dim=-1)\n",
        "                labels = batch['label_idx']\n",
        "                flat_qids = batch['quote_ids']\n",
        "                preds_list = preds.cpu().tolist()\n",
        "                labels_list = labels.cpu().tolist()\n",
        "                conf_list = conf_vals.cpu().tolist()\n",
        "            for pred, label, qid, conf in zip(preds_list, labels_list, flat_qids, conf_list):\n",
        "                adjusted_pred = apply_postprocess(pred, conf, qid)\n",
        "                meta = val_meta_map.get(qid, {})\n",
        "                eval_details['preds'].append(adjusted_pred)\n",
        "                eval_details['labels'].append(label)\n",
        "                eval_details['genres'].append(meta.get('genre', 'unknown'))\n",
        "                eval_details['confidences'].append(float(conf))\n",
        "                eval_details['qids'].append(qid)\n",
        "                eval_details['samples'].append({\n",
        "                    'qid': qid,\n",
        "                    'pred': adjusted_pred,\n",
        "                    'label': label,\n",
        "                    'genre': meta.get('genre', 'unknown'),\n",
        "                    'confidence': float(conf)\n",
        "                })\n",
        "                if label < 0:\n",
        "                    continue\n",
        "                bucket = categorize_sample(meta)\n",
        "                buckets[bucket]['total'] += 1\n",
        "                if adjusted_pred == label:\n",
        "                    buckets[bucket]['correct'] += 1\n",
        "                correct += 1 if adjusted_pred == label else 0\n",
        "                total += 1\n",
        "    overall = correct / total if total else 0.0\n",
        "    bucket_metrics = {k: (v['correct']/v['total'] if v['total'] else 0.0) for k,v in buckets.items()}\n",
        "    if is_main:\n",
        "        latest_eval_details = eval_details\n",
        "    return (overall if is_main else 0.0), (bucket_metrics if is_main else {}), eval_details\n",
        "\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"TRAINING: {CONFIG['name']}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# CURSOR: Multi-fold training loop\n",
        "for fold_idx in FOLDS_TO_TRAIN:\n",
        "    if len(FOLDS_TO_TRAIN) > 1:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"FOLD {fold_idx + 1}/{len(FOLDS_TO_TRAIN)} (split_{fold_idx})\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Reload data for this fold\n",
        "        if not use_multi:\n",
        "            train_samples, val_samples, test_samples = load_fold_data(fold_idx)\n",
        "            val_meta_map = {s['quote_id']: s for s in val_samples}\n",
        "            print(f\"   Train quotes: {len(train_samples)}\")\n",
        "            print(f\"   Val quotes: {len(val_samples)}\")\n",
        "            print(f\"   Test quotes: {len(test_samples)}\")\n",
        "            \n",
        "            # Recreate dataloaders for this fold\n",
        "            train_loader, val_loader = create_dataloaders(train_samples, val_samples, tokenizer, augmenter)\n",
        "            \n",
        "            # Re-prepare with accelerator if needed\n",
        "            if accelerator:\n",
        "                train_loader, val_loader = accelerator.prepare(train_loader, val_loader)\n",
        "        \n",
        "        # Reinitialize model for each fold\n",
        "        model = MaxPerformanceSpeakerModel(CONFIG.get('base_model', 'microsoft/deberta-v3-large'))\n",
        "        if CONFIG['gradient_checkpointing']:\n",
        "            model.encoder.gradient_checkpointing_enable()\n",
        "        \n",
        "        # Reinitialize optimizer and scheduler\n",
        "        optimizer = AdamW(model.parameters(), lr=CONFIG['lr'])\n",
        "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n",
        "        \n",
        "        if accelerator:\n",
        "            model, optimizer = accelerator.prepare(model, optimizer)\n",
        "            device = accelerator.device\n",
        "        else:\n",
        "            model = model.to(device)\n",
        "\n",
        "    global_step, best_acc, start_epoch = load_latest_checkpoint(fold_idx if len(FOLDS_TO_TRAIN) > 1 else None)\n",
        "    ce_loss = base_ce_loss\n",
        "    # CURSOR: Only run second forward pass when R-Drop is enabled (saves ~2x compute otherwise)\n",
        "    use_r_drop = CONFIG.get('r_drop_alpha', 0.0) > 0\n",
        "\n",
        "    for epoch in range(start_epoch, CONFIG['epochs']):\n",
        "        if isinstance(train_loader.sampler, CurriculumSampler):\n",
        "            train_loader.sampler.set_epoch(epoch)\n",
        "        if (not accelerator) or accelerator.is_main_process:\n",
        "            fold_str = f\" [Fold {fold_idx}]\" if len(FOLDS_TO_TRAIN) > 1 else \"\"\n",
        "            print(f\"\\n--- Epoch {epoch + 1}/{CONFIG['epochs']}{fold_str} ---\")\n",
        "        model.train()\n",
        "        accum_step = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "            if accelerator:\n",
        "                with accelerator.accumulate(model):\n",
        "                    logits1, _ = model(\n",
        "                        batch['input_ids'],\n",
        "                        batch['attention_mask'],\n",
        "                        batch['quote_mask'],\n",
        "                        batch['candidate_masks'],\n",
        "                        batch['candidate_attention_mask']\n",
        "                    )\n",
        "                    if use_r_drop:\n",
        "                        logits2, _ = model(\n",
        "                            batch['input_ids'],\n",
        "                            batch['attention_mask'],\n",
        "                            batch['quote_mask'],\n",
        "                            batch['candidate_masks'],\n",
        "                            batch['candidate_attention_mask']\n",
        "                        )\n",
        "                    else:\n",
        "                        logits2 = None\n",
        "                    if use_combined_loss and loss_fn is not None:\n",
        "                        loss = loss_fn(logits1, batch['label_idx'], logits2)\n",
        "                        loss_main = loss\n",
        "                    elif use_r_drop and logits2 is not None:\n",
        "                        loss_main = ce_loss(logits1, batch['label_idx'])\n",
        "                        loss_aux = ce_loss(logits2, batch['label_idx'])\n",
        "                        kl_pen = r_drop_penalty(logits1, logits2, CONFIG.get('r_drop_alpha', 0.0))\n",
        "                        loss = 0.5 * (loss_main + loss_aux) + kl_pen\n",
        "                    else:\n",
        "                        loss_main = ce_loss(logits1, batch['label_idx'])\n",
        "                        loss = loss_main\n",
        "                    accelerator.backward(loss)\n",
        "                    accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "            else:\n",
        "                batch = move_to_device(batch, device)\n",
        "                with autocast(enabled=CONFIG['fp16']):\n",
        "                    logits1, _ = model(\n",
        "                        batch['input_ids'],\n",
        "                        batch['attention_mask'],\n",
        "                        batch['quote_mask'],\n",
        "                        batch['candidate_masks'],\n",
        "                        batch['candidate_attention_mask']\n",
        "                    )\n",
        "                    if use_r_drop:\n",
        "                        logits2, _ = model(\n",
        "                            batch['input_ids'],\n",
        "                            batch['attention_mask'],\n",
        "                            batch['quote_mask'],\n",
        "                            batch['candidate_masks'],\n",
        "                            batch['candidate_attention_mask']\n",
        "                        )\n",
        "                    else:\n",
        "                        logits2 = None\n",
        "                    if use_combined_loss and loss_fn is not None:\n",
        "                        loss = loss_fn(logits1, batch['label_idx'], logits2)\n",
        "                        loss_main = loss\n",
        "                    elif use_r_drop and logits2 is not None:\n",
        "                        loss_main = ce_loss(logits1, batch['label_idx'])\n",
        "                        loss_aux = ce_loss(logits2, batch['label_idx'])\n",
        "                        kl_pen = r_drop_penalty(logits1, logits2, CONFIG.get('r_drop_alpha', 0.0))\n",
        "                        loss = 0.5 * (loss_main + loss_aux) + kl_pen\n",
        "                    else:\n",
        "                        loss_main = ce_loss(logits1, batch['label_idx'])\n",
        "                        loss = loss_main\n",
        "                    loss = loss / CONFIG['gradient_accumulation_steps']\n",
        "                if scaler:\n",
        "                    scaler.scale(loss).backward()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "                accum_step += 1\n",
        "                if accum_step % CONFIG['gradient_accumulation_steps'] == 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                    if scaler:\n",
        "                        scaler.step(optimizer)\n",
        "                        scaler.update()\n",
        "                    else:\n",
        "                        optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "\n",
        "            if global_step % CONFIG['checkpoint_every'] == 0:\n",
        "                save_checkpoint(global_step, {'loss': float(loss_main.detach().cpu())}, best_acc, epoch, fold_idx if len(FOLDS_TO_TRAIN) > 1 else None)\n",
        "\n",
        "            if global_step % CONFIG['eval_every'] == 0:\n",
        "                acc, buckets, latest_eval_details = evaluate()\n",
        "                if (not accelerator) or accelerator.is_main_process:\n",
        "                    print(f\"Step {global_step} | Accuracy: {acc:.4f} | buckets: {buckets}\")\n",
        "                    if acc > best_acc:\n",
        "                        best_acc = acc\n",
        "                        model_suffix = f\"_split_{fold_idx}\" if len(FOLDS_TO_TRAIN) > 1 else \"\"\n",
        "                        save_path = f\"{CONFIG['output_dir']}/best_model{model_suffix}.pt\"\n",
        "                        torch.save((accelerator.unwrap_model(model) if accelerator else model).state_dict(), save_path)\n",
        "                        print(f\"New best model! ({best_acc:.4f}) -> {save_path}\")\n",
        "                model.train()\n",
        "\n",
        "        if (not accelerator) or accelerator.is_main_process:\n",
        "            if best_acc >= CONFIG['target_accuracy']:\n",
        "                print(f\"\\nðŸŽ‰ Target accuracy {CONFIG['target_accuracy']:.0%} reached!\")\n",
        "                break\n",
        "\n",
        "    # Temperature calibration on val set after training this fold\n",
        "    best_temp = 1.0\n",
        "    if CONFIG.get('calibrate_temperature', False):\n",
        "        scaler_ts = TemperatureScaling()\n",
        "        val_logits_all = []\n",
        "        val_labels_all = []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                if accelerator:\n",
        "                    logits, _ = model(\n",
        "                        batch['input_ids'],\n",
        "                        batch['attention_mask'],\n",
        "                        batch['quote_mask'],\n",
        "                        batch['candidate_masks'],\n",
        "                        batch['candidate_attention_mask']\n",
        "                    )\n",
        "                    labels = batch['label_idx']\n",
        "                    val_logits_all.append(accelerator.gather_for_metrics(logits).detach().cpu())\n",
        "                    val_labels_all.append(accelerator.gather_for_metrics(labels).detach().cpu())\n",
        "                else:\n",
        "                    batch = move_to_device(batch, device)\n",
        "                    logits, _ = model(\n",
        "                        batch['input_ids'],\n",
        "                        batch['attention_mask'],\n",
        "                        batch['quote_mask'],\n",
        "                        batch['candidate_masks'],\n",
        "                        batch['candidate_attention_mask']\n",
        "                    )\n",
        "                    labels = batch['label_idx']\n",
        "                    val_logits_all.append(logits.detach().cpu())\n",
        "                    val_labels_all.append(labels.detach().cpu())\n",
        "        if val_logits_all:\n",
        "            val_logits = torch.cat(val_logits_all, dim=0)\n",
        "            val_labels = torch.cat(val_labels_all, dim=0)\n",
        "            scaler_ts.calibrate(val_logits, val_labels)\n",
        "            best_temp = scaler_ts.get_temperature()\n",
        "            acc_cal, buckets_cal, latest_eval_details = evaluate(temp=best_temp)\n",
        "            if (not accelerator) or accelerator.is_main_process:\n",
        "                fold_str = f\" [Fold {fold_idx}]\" if len(FOLDS_TO_TRAIN) > 1 else \"\"\n",
        "                print(f\"Calibrated temperature{fold_str}: {best_temp:.3f} (val acc={acc_cal:.4f}, buckets={buckets_cal})\")\n",
        "\n",
        "    if (not accelerator) or accelerator.is_main_process:\n",
        "        fold_str = f\" [Fold {fold_idx}]\" if len(FOLDS_TO_TRAIN) > 1 else \"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"FOLD {fold_idx} COMPLETE{fold_str}\")\n",
        "        print(f\"Best Accuracy: {best_acc:.4f}\")\n",
        "        print(f\"Temperature: {best_temp}\")\n",
        "        model_suffix = f\"_split_{fold_idx}\" if len(FOLDS_TO_TRAIN) > 1 else \"\"\n",
        "        print(f\"Model saved to: {CONFIG['output_dir']}/best_model{model_suffix}.pt\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "# Run analysis and optimization only once at the end\n",
        "if (not accelerator) or accelerator.is_main_process:\n",
        "    if CONFIG.get('run_cross_domain_validation', False) and latest_eval_details:\n",
        "        validator = CrossDomainValidator(\n",
        "            thresholds={\n",
        "                'accuracy': CONFIG.get('min_genre_acc', 0.75),\n",
        "                'f1': 0.70,\n",
        "                'min_samples': 10,\n",
        "            }\n",
        "        )\n",
        "        validator.add_predictions(\n",
        "            latest_eval_details.get('preds', []),\n",
        "            latest_eval_details.get('labels', []),\n",
        "            latest_eval_details.get('genres', [])\n",
        "        )\n",
        "        passed, domain_report = validator.validate()\n",
        "        validator.print_report()\n",
        "        if CONFIG.get('run_genre_adaptation', False):\n",
        "            adapter = GenreSpecificAdaptation(min_accuracy_threshold=CONFIG.get('min_genre_acc', 0.75))\n",
        "            adapter.underperforming_genres = domain_report.get('failing_genres', [])\n",
        "            for genre in domain_report.get('failing_genres', []):\n",
        "                print(f\"Genre {genre}: suggested fallback -> {adapter.get_adaptation_strategy(genre)}\")\n",
        "    if CONFIG.get('run_error_analysis', False) and latest_eval_details:\n",
        "        analyzer = ErrorAnalyzer()\n",
        "        for sample in latest_eval_details.get('samples', []):\n",
        "            if sample['label'] < 0 or sample['pred'] == sample['label']:\n",
        "                continue\n",
        "            meta = val_meta_map.get(sample['qid'], {})\n",
        "            candidates = meta.get('candidates', [])\n",
        "            predicted_name = candidates[sample['pred']] if 0 <= sample['pred'] < len(candidates) else str(sample['pred'])\n",
        "            actual_name = candidates[sample['label']] if 0 <= sample['label'] < len(candidates) else str(sample['label'])\n",
        "            analyzer.add_error(\n",
        "                sample_id=sample['qid'],\n",
        "                text=meta.get('text', ''),\n",
        "                quote=meta.get('text', ''),\n",
        "                predicted=predicted_name,\n",
        "                actual=actual_name,\n",
        "                candidates=candidates,\n",
        "                confidence=sample.get('confidence', 0.0),\n",
        "                genre=meta.get('genre', 'unknown')\n",
        "            )\n",
        "        summary = analyzer.get_summary()\n",
        "        recs = analyzer.get_recommendations()\n",
        "        print(\"Error analysis summary:\", summary)\n",
        "        if recs:\n",
        "            print(\"Recommendations:\")\n",
        "            for rec in recs:\n",
        "                print(f\"- {rec}\")\n",
        "\n",
        "if (not accelerator) or accelerator.is_main_process:\n",
        "    if CONFIG.get('run_model_optimization', False):\n",
        "        try:\n",
        "            print(\"Running model optimization for inference...\")\n",
        "            opt_model = accelerator.unwrap_model(model) if accelerator else model\n",
        "            opt_model = opt_model.to('cpu')\n",
        "            optimize_for_inference(\n",
        "                model=opt_model,\n",
        "                output_dir=f\"{CONFIG['output_dir']}/optimized\",\n",
        "                quantize=CONFIG.get('optimize_quantize', True),\n",
        "                export_onnx=CONFIG.get('optimize_export_onnx', False),\n",
        "                calibration_data=None\n",
        "            )\n",
        "        except Exception as exc:\n",
        "            print(f\"Model optimization failed: {exc}\")\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ALL TRAINING COMPLETE!\")\n",
        "    if len(FOLDS_TO_TRAIN) > 1:\n",
        "        print(f\"Trained {len(FOLDS_TO_TRAIN)} folds: {FOLDS_TO_TRAIN}\")\n",
        "        print(f\"Models saved: best_model_split_{{0-{len(FOLDS_TO_TRAIN)-1}}}.pt\")\n",
        "    else:\n",
        "        print(f\"Trained single fold: {FOLDS_TO_TRAIN[0]}\")\n",
        "        print(f\"Model saved: best_model.pt\")\n",
        "    print(f\"Output dir: {CONFIG['output_dir']}\")\n",
        "    print(f\"{'='*60}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
