{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BookNLP Maximum Performance Quote Attribution (Unified)\n",
    "\n",
    "**Goal**: Train the max-performance quote attribution model (8090% accuracy) in either Kaggle or Colab via one RUN_ENV-aware notebook.\n",
    "\n",
    "**Features**\n",
    "- DeBERTa-v3-large with quote/candidate masks + [QUOTE], [ALTQUOTE], [PAR]\n",
    "- Candidate-level softmax with label smoothing; optional R-Drop; optional temperature scaling\n",
    "- Optional multi-source loading + genre-balanced sampler; PDNC fallback; configurable hard negatives\n",
    "- Curriculum sampler + light augmentation; gradient checkpointing + FP16\n",
    "- Auto checkpoint/resume (model/optimizer/scheduler/best_acc) with cadence set by RUN_ENV; bucketed eval + placeholder postprocess hook\n",
    "\n",
    "**Requirements**\n",
    "- Kaggle: T4 x2 accelerator; storage under `/kaggle/working`\n",
    "- Colab: T4 GPU; storage in Drive at `/content/drive`\n",
    "\n",
    "**Quick start**\n",
    "1) Set `RUN_ENV = \"kaggle\"` or `\"colab\"` in the next cell (default: kaggle).\n",
    "2) Kaggle: no Drive mount; repo/output in `/kaggle/working`; multi-GPU via `accelerate`; checkpoints/evals every 500 steps; auto-resume from latest `checkpoint_*.pt`.\n",
    "3) Colab: mounts Drive to `/content/drive`; repo in `/content`; outputs in Drive; single-GPU (no DDP) with gradient accumulation; checkpoints/evals every 300 steps; auto-resume from latest `checkpoint_*.pt`.\n",
    "4) Run all cellsdata is cloned automatically from the repo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN_ENV toggle\n",
    "Set `RUN_ENV = \"kaggle\"` or `\"colab\"` in the next cell (default: kaggle). Paths, checkpoint cadence, and mounts adjust automatically. Kaggle uses multi-GPU via `accelerate` (no Drive mount); Colab mounts Drive, runs single-GPU with gradient accumulation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, torch\n",
    "\n",
    "# CURSOR: Toggle once; everything else keys off this value\n",
    "RUN_ENV = os.environ.get(\"RUN_ENV\", \"kaggle\").strip().lower()\n",
    "ENV_CFG = {\n",
    "    \"kaggle\": {\n",
    "        \"base_dir\": \"/kaggle/working\",\n",
    "        \"repo_dir\": \"/kaggle/working/speaker-attribution-acl2023\",\n",
    "        \"training_repo_dir\": \"/kaggle/working/quote-attribution-training\",\n",
    "        \"output_root\": \"/kaggle/working\",\n",
    "        \"checkpoint_every\": 500,\n",
    "        \"eval_every\": 500,\n",
    "        \"grad_accum\": 8,\n",
    "        \"mount_drive\": False,\n",
    "    },\n",
    "    \"colab\": {\n",
    "        \"base_dir\": \"/content/drive/MyDrive/quote_attribution\",\n",
    "        \"repo_dir\": \"/content/speaker-attribution-acl2023\",\n",
    "        \"training_repo_dir\": \"/content/quote-attribution-training\",\n",
    "        \"output_root\": \"/content/drive/MyDrive/quote_attribution\",\n",
    "        \"checkpoint_every\": 300,\n",
    "        \"eval_every\": 300,\n",
    "        \"grad_accum\": 16,\n",
    "        \"mount_drive\": True,\n",
    "    },\n",
    "}\n",
    "assert RUN_ENV in ENV_CFG, f\"Unsupported RUN_ENV: {RUN_ENV}\"\n",
    "ENV = ENV_CFG[RUN_ENV]\n",
    "\n",
    "if ENV[\"mount_drive\"]:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    os.makedirs(ENV[\"base_dir\"], exist_ok=True)\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    raise RuntimeError(\"GPU not available; enable a GPU runtime.\")\n",
    "\n",
    "# Data root (datasets auto-downloaded later based on CONFIG['datasets'])\n",
    "REPO_DIR = ENV[\"repo_dir\"]\n",
    "os.makedirs(REPO_DIR, exist_ok=True)\n",
    "print(f\"Data repo root: {REPO_DIR}\")\n",
    "\n",
    "# Clone training code repository\n",
    "TRAINING_REPO_DIR = ENV[\"training_repo_dir\"]\n",
    "if not os.path.exists(TRAINING_REPO_DIR):\n",
    "    print(\"\\n[DOWN] Cloning training code repository...\")\n",
    "    !git clone https://github.com/bohdan-natsevych/quote-attribution-training.git {TRAINING_REPO_DIR}\n",
    "else:\n",
    "    print(f\"[OK] Training repository present at {TRAINING_REPO_DIR}\")\n",
    "\n",
    "# Add training repo to Python path for imports\n",
    "if TRAINING_REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, TRAINING_REPO_DIR)\n",
    "    print(f\"[OK] Added {TRAINING_REPO_DIR} to Python path\")\n",
    "\n",
    "DATA_ROOT = f\"{REPO_DIR}/data\"\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "\n",
    "BASE_DIR = ENV[\"base_dir\"]\n",
    "OUTPUT_ROOT = ENV[\"output_root\"]\n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "print(f\"Output root: {OUTPUT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers>=4.30.0 accelerate>=0.20.0 datasets scikit-learn tqdm nlpaug nltk matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# CURSOR: Import to get available datasets\n",
    "import sys\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Union\n",
    "if TRAINING_REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, TRAINING_REPO_DIR)\n",
    "\n",
    "from data.multi_source_data import MultiSourceDataLoader\n",
    "\n",
    "# Define Dataset enum from available datasets\n",
    "class Dataset(str, Enum):\n",
    "    \"\"\"Available datasets for quote attribution training.\"\"\"\n",
    "    PDNC = \"pdnc\"\n",
    "    LITBANK = \"litbank\"\n",
    "    DIRECTQUOTE = \"directquote\"\n",
    "    # CURSOR: QUOTEBANK = \"quotebank\"\n",
    "    \n",
    "    @classmethod\n",
    "    def get_all(cls):\n",
    "        \"\"\"Get all dataset values.\"\"\"\n",
    "        return [d.value for d in cls]\n",
    "    \n",
    "    @classmethod\n",
    "    def validate(cls, datasets: list):\n",
    "        \"\"\"Validate that all datasets are in the enum.\"\"\"\n",
    "        valid = cls.get_all()\n",
    "        for ds in datasets:\n",
    "            if ds not in valid:\n",
    "                raise ValueError(f\"Invalid dataset '{ds}'. Must be one of: {valid}\")\n",
    "        return True\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Validated configuration for quote attribution training.\"\"\"\n",
    "    \n",
    "    # Core settings\n",
    "    target_level: int\n",
    "    name: str\n",
    "    target_accuracy: float\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    epochs: int = 15\n",
    "    batch_size: int = 8\n",
    "    lr: float = 5e-6\n",
    "    \n",
    "    # Dataset configuration\n",
    "    datasets: List[str] = field(default_factory=lambda: ['pdnc'])\n",
    "    fold_selection: Union[str, List[int]] = \"all\"\n",
    "    \n",
    "    # Advanced features\n",
    "    use_augmentation: bool = True\n",
    "    use_curriculum: bool = True\n",
    "    focal_gamma: float = 2.0\n",
    "    label_smoothing: float = 0.1\n",
    "    r_drop_alpha: float = 0.0\n",
    "    hard_negative_topk: int = 2\n",
    "    max_candidates: int = 10\n",
    "    shuffle_candidates: bool = True\n",
    "    calibrate_temperature: bool = True\n",
    "    balance_genres: bool = False\n",
    "    min_genre_acc: float = 0.75\n",
    "    \n",
    "    # Model configuration (populated after init)\n",
    "    base_model: str = 'microsoft/deberta-v3-large'\n",
    "    max_length: int = 512\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    checkpoint_every: int = 500\n",
    "    eval_every: int = 500\n",
    "    fp16: bool = True\n",
    "    gradient_checkpointing: bool = True\n",
    "    seed: int = 42\n",
    "    output_dir: str = ''\n",
    "    multi_source_base: str = ''\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate configuration after initialization.\"\"\"\n",
    "        # Validate target level\n",
    "        if self.target_level == 3:\n",
    "            raise NotImplementedError(\n",
    "                \"TARGET_LEVEL=3 ensemble training not implemented in notebook. \"\n",
    "                \"Train 5 folds with TARGET_LEVEL=2, then use models/ensemble.py for ensemble inference.\"\n",
    "            )\n",
    "        if self.target_level not in [1, 2]:\n",
    "            raise ValueError(f\"target_level must be 1 or 2, got {self.target_level}\")\n",
    "        \n",
    "        # Validate datasets\n",
    "        Dataset.validate(self.datasets)\n",
    "        \n",
    "        # Validate hyperparameters\n",
    "        if self.batch_size < 1:\n",
    "            raise ValueError(f\"batch_size must be >= 1, got {self.batch_size}\")\n",
    "        if self.epochs < 1:\n",
    "            raise ValueError(f\"epochs must be >= 1, got {self.epochs}\")\n",
    "        if not 0 <= self.label_smoothing <= 0.5:\n",
    "            raise ValueError(f\"label_smoothing must be in [0, 0.5], got {self.label_smoothing}\")\n",
    "        if self.focal_gamma < 0:\n",
    "            raise ValueError(f\"focal_gamma must be >= 0, got {self.focal_gamma}\")\n",
    "        \n",
    "        if self.hard_negative_topk < 0:\n",
    "            raise ValueError(f\"hard_negative_topk must be >= 0, got {self.hard_negative_topk}\")\n",
    "        if self.max_candidates < 2:\n",
    "            raise ValueError(f\"max_candidates must be >= 2, got {self.max_candidates}\")\n",
    "        \n",
    "        # Validate fold selection\n",
    "        if isinstance(self.fold_selection, list):\n",
    "            if not all(isinstance(f, int) and 0 <= f < 5 for f in self.fold_selection):\n",
    "                raise ValueError(f\"fold_selection list must contain integers in [0, 4], got {self.fold_selection}\")\n",
    "        elif self.fold_selection != \"all\":\n",
    "            raise ValueError(f\"fold_selection must be 'all' or list of ints, got {self.fold_selection}\")\n",
    "\n",
    "\n",
    "# Configuration selection\n",
    "TARGET_LEVEL = 1  # 1=PDNC, 2=multi-source\n",
    "FOLD_SELECTION = [0,1]\n",
    "\n",
    "# Define configurations\n",
    "if TARGET_LEVEL == 1:\n",
    "    CONFIG = TrainingConfig(\n",
    "        target_level=1,\n",
    "        name='Target 1: DeBERTa-large + Augmentation',\n",
    "        target_accuracy=0.85,\n",
    "        epochs=6,  # CURSOR: Reduced from 15 to fit 5 folds in 30 hours on 2xT4\n",
    "        batch_size=4,\n",
    "        lr=5e-6,\n",
    "        datasets=['pdnc'],\n",
    "        fold_selection=FOLD_SELECTION,\n",
    "        use_augmentation=True,\n",
    "        use_curriculum=True,\n",
    "        focal_gamma=2.0,\n",
    "        label_smoothing=0.1,\n",
    "        hard_negative_topk=2,\n",
    "        calibrate_temperature=True,\n",
    "        balance_genres=False,\n",
    "    )\n",
    "elif TARGET_LEVEL == 2:\n",
    "    CONFIG = TrainingConfig(\n",
    "        target_level=2,\n",
    "        name='Target 2: Multi-Source + Genre Balancing',\n",
    "        target_accuracy=0.88,\n",
    "        epochs=8,  # CURSOR: Reduced from 15 for reasonable training time\n",
    "        batch_size=4,\n",
    "        lr=2e-6,\n",
    "        datasets=['pdnc', 'litbank', 'directquote'],\n",
    "        fold_selection=FOLD_SELECTION,\n",
    "        use_augmentation=True,\n",
    "        use_curriculum=True,\n",
    "        focal_gamma=2.0,\n",
    "        label_smoothing=0.1,\n",
    "        hard_negative_topk=2,\n",
    "        calibrate_temperature=True,\n",
    "        balance_genres=True,\n",
    "        min_genre_acc=0.75,\n",
    "    )\n",
    "else:\n",
    "    # This will raise NotImplementedError in __post_init__\n",
    "    CONFIG = TrainingConfig(\n",
    "        target_level=TARGET_LEVEL,\n",
    "        name='Target 3: Ensemble (Not Implemented)',\n",
    "        target_accuracy=0.90,\n",
    "    )\n",
    "\n",
    "# Update environment-specific settings\n",
    "multi_source_base = f\"{REPO_DIR}/data\"\n",
    "CONFIG.multi_source_base = multi_source_base\n",
    "CONFIG.gradient_accumulation_steps = ENV['grad_accum']\n",
    "CONFIG.checkpoint_every = ENV['checkpoint_every']\n",
    "CONFIG.eval_every = ENV['eval_every']\n",
    "CONFIG.output_dir = f\"{OUTPUT_ROOT}/target_{TARGET_LEVEL}\"\n",
    "\n",
    "os.makedirs(CONFIG.output_dir, exist_ok=True)\n",
    "print(f\"Selected: {CONFIG.name}\")\n",
    "print(f\"Target accuracy: {CONFIG.target_accuracy:.0%}\")\n",
    "print(f\"Datasets: {CONFIG.datasets}\")\n",
    "print(f\"Output dir: {CONFIG.output_dir}\")\n",
    "print(f\"RUN_ENV: {RUN_ENV} | checkpoint_every={CONFIG.checkpoint_every} | grad_accum={CONFIG.gradient_accumulation_steps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# AUTO-DOWNLOAD AND PREPARE ALL DATASETS\n",
    "# =============================================================================\n",
    "\n",
    "from data.multi_source_data import download_datasets\n",
    "\n",
    "downloaded_datasets = download_datasets(\n",
    "    base_path=CONFIG.multi_source_base,\n",
    "    datasets=CONFIG.datasets\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GPU SETUP - DataParallel for multi-GPU on Kaggle (no process spawning)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "print(f\"[SEARCH] Detected {NUM_GPUS} GPU(s)\")\n",
    "for i in range(NUM_GPUS):\n",
    "    props = torch.cuda.get_device_properties(i)\n",
    "    print(f\"   GPU {i}: {torch.cuda.get_device_name(i)} ({props.total_memory / 1024**3:.1f} GB)\")\n",
    "\n",
    "# CURSOR: Use DataParallel for multi-GPU on Kaggle - runs in single process\n",
    "# CURSOR: Avoids all DDP/DeepSpeed process spawning issues\n",
    "USE_DATA_PARALLEL = NUM_GPUS > 1\n",
    "\n",
    "if USE_DATA_PARALLEL:\n",
    "    print(f\"\\n[OK] Multi-GPU training via DataParallel (single process)\")\n",
    "    print(f\"   Effective batch: {CONFIG.batch_size} x {NUM_GPUS} x {CONFIG.gradient_accumulation_steps} = {CONFIG.batch_size * NUM_GPUS * CONFIG.gradient_accumulation_steps}\")\n",
    "else:\n",
    "    print(f\"\\n[OK] Single-GPU training mode\") \n",
    "    print(f\"   Effective batch: {CONFIG.batch_size} x 1 x {CONFIG.gradient_accumulation_steps} = {CONFIG.batch_size * CONFIG.gradient_accumulation_steps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, random, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "import time\n",
    "import csv\n",
    "import logging\n",
    "import sys\n",
    "from datetime import timedelta\n",
    "\n",
    "# CURSOR: Multi-GPU stability env vars - must be set BEFORE torch import\n",
    "import os\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"           # CURSOR: Disable P2P for T4 compatibility\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"            # CURSOR: Disable InfiniBand (not available on Kaggle)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # CURSOR: Avoid tokenizer fork issues with DDP\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "os.environ.setdefault(\"USE_TF\", \"0\")  # CURSOR: Avoid TF/Keras imports; notebook uses PyTorch-only Trainer.\n",
    "\n",
    "# CURSOR: Configure logging to display in Jupyter/Kaggle notebooks\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    datefmt='%H:%M:%S',\n",
    "    level=logging.INFO,\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    force=True  # CURSOR: Force reconfiguration even if logging was already set up\n",
    ")\n",
    "\n",
    "# CURSOR: Configure tqdm for notebook display\n",
    "from tqdm.auto import tqdm\n",
    "os.environ[\"TQDM_NOTEBOOK\"] = \"1\"\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, EvalPrediction\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_info()  # CURSOR: Ensure transformers logs are visible\n",
    "hf_logging.enable_progress_bar()  # CURSOR: Enable progress bar in transformers\n",
    "\n",
    "# CURSOR: Force HuggingFace Hub to show download progress\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"0\"\n",
    "\n",
    "print(\"[OK] Logging configured - you should see output below during training\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from data.multi_source_data import MultiSourceDataLoader\n",
    "from data.data_augmentation import QuoteAugmenter\n",
    "from data.curriculum_loader import DifficultyClassifier, CurriculumSampler, CurriculumConfig\n",
    "from evaluation.confidence_calibration import TemperatureScaling\n",
    "from evaluation.error_analysis import ErrorAnalyzer\n",
    "from optimization.post_processing import PostProcessor\n",
    "from models.max_performance_model import MaxPerformanceSpeakerModel\n",
    "from losses.focal_loss import CombinedLoss\n",
    "\n",
    "# CURSOR: Try to import wandb, but don't fail if unavailable\n",
    "try:\n",
    "    import wandb\n",
    "    WANDB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WANDB_AVAILABLE = False\n",
    "    print(\"[WARN]  wandb not available, logging to CSV only\")\n",
    "\n",
    "# CURSOR: Deterministic setup for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(CONFIG.seed)\n",
    "print(f\"GPUs: {NUM_GPUS} | FP16: {CONFIG.fp16}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATASET CLASS AND DATA LOADING\n",
    "# =============================================================================\n",
    "\n",
    "from data.training_samples import finalize_candidate_sets, derive_quote_span\n",
    "\n",
    "class QuoteDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for quote attribution with candidate masking.\n",
    "    Handles tokenization, augmentation, and candidate mask generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, samples, tokenizer, max_length=512, augment=False, augmenter: QuoteAugmenter = None):\n",
    "        self.samples, self.tok, self.max_len = samples, tokenizer, max_length\n",
    "        self.augment = augment\n",
    "        self.augmenter = augmenter\n",
    "        self.par_id = self.tok.convert_tokens_to_ids(\"[PAR]\")\n",
    "        self.altq_id = self.tok.convert_tokens_to_ids(\"[ALTQUOTE]\")\n",
    "        self.quote_id = self.tok.convert_tokens_to_ids(\"[QUOTE]\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _maybe_augment_text(self, sample: dict) -> str:\n",
    "        text = sample.get('text', '')\n",
    "        if not self.augment or not self.augmenter:\n",
    "            return text\n",
    "\n",
    "        # Apply augmentation with 50% probability\n",
    "        if random.random() > 0.5:\n",
    "            return text\n",
    "\n",
    "        quote = sample.get('quote', '')\n",
    "        q_start = sample.get('quote_start')\n",
    "        q_end = sample.get('quote_end')\n",
    "        if quote and (q_start is None or q_end is None):\n",
    "            q_start, q_end = derive_quote_span(text, quote)\n",
    "\n",
    "        protected = []\n",
    "        if q_start is not None and q_end is not None and q_start >= 0 and q_end > q_start:\n",
    "            protected = [(int(q_start), int(q_end))]\n",
    "\n",
    "        augmented = text\n",
    "        try:\n",
    "            if random.random() < 0.4:\n",
    "                n_synonyms = random.randint(3, 5)\n",
    "                augmented = self.augmenter.synonym_replace(augmented, protected_spans=protected, n=n_synonyms)\n",
    "            if random.random() < 0.25:\n",
    "                augmented = self.augmenter.random_insert(augmented, protected_spans=protected, n=1)\n",
    "            if random.random() < 0.2:\n",
    "                augmented = self.augmenter.random_swap(augmented, protected_spans=protected, n=1)\n",
    "            if random.random() < 0.15:\n",
    "                augmented = self.augmenter.random_delete(augmented, protected_spans=protected, p=0.03)\n",
    "            return augmented\n",
    "        except Exception:\n",
    "            return text\n",
    "\n",
    "    def _ensure_quote_markers(self, text: str, quote: str, q_start: Optional[int], q_end: Optional[int]) -> str:\n",
    "        if not quote:\n",
    "            return text\n",
    "\n",
    "        if q_start is not None and q_end is not None and 0 <= q_start < q_end <= len(text):\n",
    "            return text[:q_start] + \" [QUOTE] \" + text[q_start:q_end] + \" [QUOTE] \" + text[q_end:]\n",
    "\n",
    "        # Best-effort substring match\n",
    "        pos = text.find(quote)\n",
    "        if pos < 0:\n",
    "            pos = text.lower().find(quote.lower())\n",
    "        if pos >= 0:\n",
    "            end = pos + len(quote)\n",
    "            return text[:pos] + \" [QUOTE] \" + text[pos:end] + \" [QUOTE] \" + text[end:]\n",
    "\n",
    "        # Fallback: append quote with markers\n",
    "        return text + \" [QUOTE] \" + quote + \" [QUOTE]\"\n",
    "\n",
    "    def _truncate_around_quote(self, base_ids: list, room: int) -> list:\n",
    "        if len(base_ids) <= room:\n",
    "            return base_ids\n",
    "\n",
    "        qpos = [i for i, tid in enumerate(base_ids) if tid == self.quote_id]\n",
    "        if len(qpos) >= 2:\n",
    "            q0, q1 = qpos[0], qpos[1]\n",
    "            span_len = (q1 - q0) + 1\n",
    "\n",
    "            if span_len >= room:\n",
    "                start = max(0, q0)\n",
    "                end = min(len(base_ids), start + room)\n",
    "                return base_ids[start:end]\n",
    "\n",
    "            left_ctx = (room - span_len) // 2\n",
    "            start = max(0, q0 - left_ctx)\n",
    "            end = start + room\n",
    "            if end <= q1:\n",
    "                end = q1 + 1\n",
    "                start = max(0, end - room)\n",
    "            end = min(len(base_ids), end)\n",
    "            start = max(0, end - room)\n",
    "            return base_ids[start:end]\n",
    "\n",
    "        return base_ids[:room]\n",
    "\n",
    "    def _build_quote_mask(self, base_ids: list, quote: str) -> list:\n",
    "        mask = [0] * len(base_ids)\n",
    "        qpos = [i for i, tid in enumerate(base_ids) if tid == self.quote_id]\n",
    "        if len(qpos) >= 2:\n",
    "            for i in range(qpos[0] + 1, qpos[1]):\n",
    "                mask[i] = 1\n",
    "            return mask\n",
    "\n",
    "        # Fallback: subsequence match for quote token ids\n",
    "        if quote:\n",
    "            q_ids = self.tok.encode(quote, add_special_tokens=False)\n",
    "            if q_ids and len(q_ids) <= len(base_ids):\n",
    "                for i in range(0, len(base_ids) - len(q_ids) + 1):\n",
    "                    if base_ids[i:i+len(q_ids)] == q_ids:\n",
    "                        for j in range(i, i + len(q_ids)):\n",
    "                            mask[j] = 1\n",
    "                        break\n",
    "\n",
    "        if sum(mask) == 0:\n",
    "            mask = [1] * len(base_ids)\n",
    "        return mask\n",
    "\n",
    "    def _encode(self, sample):\n",
    "        # CURSOR: Augment context only; keep quote span intact via protected spans.\n",
    "        context_text = self._maybe_augment_text(sample)\n",
    "        quote = sample.get('quote', '')\n",
    "\n",
    "        q_start = sample.get('quote_start')\n",
    "        q_end = sample.get('quote_end')\n",
    "        if quote and (q_start is None or q_end is None):\n",
    "            q_start, q_end = derive_quote_span(context_text, quote)\n",
    "\n",
    "        base_text = self._ensure_quote_markers(context_text, quote, q_start, q_end)\n",
    "        base_ids = self.tok.encode(base_text, add_special_tokens=False)\n",
    "\n",
    "        candidates = sample['candidates']\n",
    "        cand_ids = [self.tok.encode(c, add_special_tokens=False) for c in candidates]\n",
    "\n",
    "        reserved = 1 + 1 + sum(1 + len(ci) for ci in cand_ids)\n",
    "        room = max(self.max_len - reserved, 8)\n",
    "        base_ids = self._truncate_around_quote(base_ids, room)\n",
    "\n",
    "        base_quote_mask = self._build_quote_mask(base_ids, quote)\n",
    "\n",
    "        tokens = [self.par_id] + base_ids + [self.altq_id]\n",
    "        quote_mask = [0] + base_quote_mask + [0]\n",
    "\n",
    "        # Track candidate start/end positions\n",
    "        cand_spans = []\n",
    "        for ci in cand_ids:\n",
    "            tokens.append(self.par_id)\n",
    "            quote_mask.append(0)\n",
    "            start = len(tokens)\n",
    "            tokens.extend(ci)\n",
    "            quote_mask.extend([0] * len(ci))\n",
    "            end = len(tokens)\n",
    "            cand_spans.append((start, end))\n",
    "\n",
    "        if not cand_spans:\n",
    "            tokens.append(self.par_id)\n",
    "            quote_mask.append(0)\n",
    "            cand_spans.append((len(tokens), len(tokens)))\n",
    "\n",
    "        # Create masks with uniform length\n",
    "        final_len = len(tokens)\n",
    "        cand_masks = []\n",
    "        for start, end in cand_spans:\n",
    "            mask = [0] * final_len\n",
    "            for i in range(start, min(end, final_len)):\n",
    "                mask[i] = 1\n",
    "            cand_masks.append(mask)\n",
    "\n",
    "        # Extend quote_mask to match final token length\n",
    "        if len(quote_mask) < final_len:\n",
    "            quote_mask += [0] * (final_len - len(quote_mask))\n",
    "\n",
    "        tokens = tokens[: self.max_len]\n",
    "        quote_mask = quote_mask[: self.max_len]\n",
    "        attention = [1] * len(tokens)\n",
    "        if len(tokens) < self.max_len:\n",
    "            pad_len = self.max_len - len(tokens)\n",
    "            tokens += [self.tok.pad_token_id] * pad_len\n",
    "            attention += [0] * pad_len\n",
    "            quote_mask += [0] * pad_len\n",
    "            cand_masks = [cm + [0] * pad_len for cm in cand_masks]\n",
    "        else:\n",
    "            cand_masks = [cm[: self.max_len] for cm in cand_masks]\n",
    "\n",
    "        return tokens, attention, quote_mask, cand_masks\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        tokens, attention, quote_mask, cand_masks = self._encode(sample)\n",
    "        label_idx = sample['gold_index'] if sample['gold_index'] >= 0 else -100\n",
    "        return {\n",
    "            'input_ids': torch.tensor(tokens, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention, dtype=torch.long),\n",
    "            'quote_mask': torch.tensor(quote_mask, dtype=torch.long),\n",
    "            'candidate_masks': [torch.tensor(cm, dtype=torch.long) for cm in cand_masks],\n",
    "            'label_idx': torch.tensor(label_idx, dtype=torch.long),\n",
    "            'quote_id': sample['quote_id']\n",
    "        }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING HELPERS WITH ERROR HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "# CURSOR: Candidate construction/shuffling is handled by finalize_candidate_sets(...) for all datasets.\n",
    "\n",
    "\n",
    "def _convert_multi_source_sample(s, idx, source='unknown'):\n",
    "    \"\"\"Convert MultiSourceDataLoader sample format to QuoteDataset format.\"\"\"\n",
    "    gold = s.get('speaker', '') or s.get('gold', '')\n",
    "    text = s.get('text') or ''\n",
    "    quote = s.get('quote') or ''\n",
    "    quote_start = s.get('quote_start')\n",
    "    quote_end = s.get('quote_end')\n",
    "\n",
    "    if not text:\n",
    "        text = quote or ''\n",
    "\n",
    "    source = s.get('source', source)\n",
    "    genre = s.get('genre', source)\n",
    "    book_id = s.get('book_id', '')\n",
    "\n",
    "    if not gold or not text:\n",
    "        return None\n",
    "\n",
    "    qid = f\"{source}:{book_id}:{idx}\" if book_id else f\"{source}:{idx}\"\n",
    "\n",
    "    out = {\n",
    "        'quote_id': qid,\n",
    "        'text': text,\n",
    "        'quote': quote,\n",
    "        'quote_start': quote_start,\n",
    "        'quote_end': quote_end,\n",
    "        'gold': gold,\n",
    "        'genre': genre,\n",
    "        'source': source,\n",
    "        'book_id': book_id,\n",
    "    }\n",
    "\n",
    "    # CURSOR: Preserve provided candidate sets (e.g., PDNC) when available.\n",
    "    if isinstance(s.get('candidates'), list) and s.get('candidates'):\n",
    "        out['candidates'] = s.get('candidates')\n",
    "        out['gold_index'] = s.get('gold_index', -1)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "class PDNCFoldIterator:\n",
    "    \"\"\"Lazy loading iterator for PDNC folds. Loads folds on-demand to save memory.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str, n_folds: int = 5, seed: int = 42, validate_on_init: bool = True):\n",
    "        self.base_path = base_path\n",
    "        self.n_folds = n_folds\n",
    "        self.seed = seed\n",
    "        self._book_assignments = None\n",
    "        self._all_samples = None\n",
    "        \n",
    "        # CURSOR: Pre-flight validation - catch data issues early before training starts\n",
    "        if validate_on_init:\n",
    "            self._preflight_validation()\n",
    "    \n",
    "    def _preflight_validation(self):\n",
    "        \"\"\"Validate data is accessible before training starts.\"\"\"\n",
    "        print(f\"   [SEARCH] Running pre-flight data validation...\")\n",
    "        \n",
    "        # Check base path exists\n",
    "        from pathlib import Path\n",
    "        if not Path(self.base_path).exists():\n",
    "            raise RuntimeError(\n",
    "                f\"Data path does not exist: {self.base_path}. \"\n",
    "                \"Run dataset download cell first.\"\n",
    "            )\n",
    "        \n",
    "        # Check PDNC subdirectory exists\n",
    "        pdnc_path = Path(self.base_path) / 'pdnc'\n",
    "        if not pdnc_path.exists():\n",
    "            raise RuntimeError(\n",
    "                f\"PDNC dataset not found at {pdnc_path}. \"\n",
    "                \"Ensure dataset download completed successfully.\"\n",
    "            )\n",
    "        \n",
    "        # Try loading a small sample to validate format\n",
    "        try:\n",
    "            loader = MultiSourceDataLoader(base_path=self.base_path, datasets=['pdnc'], seed=self.seed)\n",
    "            loader.load_all()\n",
    "            \n",
    "            total_samples = sum(len(samples) for samples in loader.data_by_genre.values())\n",
    "            if total_samples == 0:\n",
    "                raise RuntimeError(\"PDNC loader returned 0 samples - check dataset format\")\n",
    "            \n",
    "            # Check book distribution for fold viability\n",
    "            by_book = defaultdict(int)\n",
    "            for genre_samples in loader.data_by_genre.values():\n",
    "                for s in genre_samples:\n",
    "                    by_book[s.get('book_id', 'unknown')] += 1\n",
    "            \n",
    "            n_books = len(by_book)\n",
    "            if n_books < self.n_folds:\n",
    "                raise ValueError(\n",
    "                    f\"Only {n_books} books found, but {self.n_folds} folds requested. \"\n",
    "                    \"Reduce n_folds or check dataset.\"\n",
    "                )\n",
    "            \n",
    "            print(f\"   [OK] Pre-flight passed: {total_samples:,} samples from {n_books} books\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Pre-flight data validation failed: {e}. \"\n",
    "                \"Check dataset download and format.\"\n",
    "            ) from e\n",
    "    \n",
    "    def _load_pdnc_data(self):\n",
    "        \"\"\"Load PDNC data using MultiSourceDataLoader.\"\"\"\n",
    "        if self._all_samples is not None:\n",
    "            return\n",
    "        \n",
    "        loader = MultiSourceDataLoader(base_path=self.base_path, datasets=['pdnc'], seed=self.seed)\n",
    "        loader.load_all()\n",
    "        \n",
    "        all_samples = []\n",
    "        for genre_samples in loader.data_by_genre.values():\n",
    "            all_samples.extend(genre_samples)\n",
    "        \n",
    "        if not all_samples:\n",
    "            raise RuntimeError(\n",
    "                f\"Failed to load PDNC data from {self.base_path}. \"\n",
    "                \"Check that dataset download completed successfully.\"\n",
    "            )\n",
    "        \n",
    "        self._all_samples = all_samples\n",
    "        \n",
    "        # Group samples by book_id for leave-book-out cross-validation\n",
    "        by_book = defaultdict(list)\n",
    "        for i, s in enumerate(all_samples):\n",
    "            book_id = s.get('book_id', 'unknown')\n",
    "            by_book[book_id].append((i, s))\n",
    "        \n",
    "        book_ids = sorted(by_book.keys())\n",
    "        n_books = len(book_ids)\n",
    "        \n",
    "        if n_books < self.n_folds:\n",
    "            raise ValueError(\n",
    "                f\"Only {n_books} books found, but {self.n_folds} folds requested. \"\n",
    "                f\"Reduce n_folds or check dataset.\"\n",
    "            )\n",
    "        \n",
    "        # Assign books to folds\n",
    "        random.seed(self.seed)\n",
    "        shuffled_books = book_ids.copy()\n",
    "        random.shuffle(shuffled_books)\n",
    "        \n",
    "        fold_book_assignments = [[] for _ in range(self.n_folds)]\n",
    "        for i, book_id in enumerate(shuffled_books):\n",
    "            fold_book_assignments[i % self.n_folds].append(book_id)\n",
    "        \n",
    "        self._book_assignments = {\n",
    "            'fold_books': fold_book_assignments,\n",
    "            'by_book': by_book,\n",
    "            'all_books': shuffled_books\n",
    "        }\n",
    "    \n",
    "    def load_fold(self, fold_idx: int, other_datasets: list = None) -> Tuple[list, list, list]:\n",
    "        \"\"\"Load a specific fold on-demand.\"\"\"\n",
    "        # Validate fold index\n",
    "        if fold_idx < 0 or fold_idx >= self.n_folds:\n",
    "            raise ValueError(\n",
    "                f\"Fold index {fold_idx} out of range [0, {self.n_folds-1}]\"\n",
    "            )\n",
    "        \n",
    "        # Ensure data is loaded\n",
    "        self._load_pdnc_data()\n",
    "        \n",
    "        # Get book assignments for this fold\n",
    "        fold_books = self._book_assignments['fold_books']\n",
    "        by_book = self._book_assignments['by_book']\n",
    "        all_books = self._book_assignments['all_books']\n",
    "        \n",
    "        test_books = set(fold_books[fold_idx])\n",
    "        val_books = set(fold_books[(fold_idx + 1) % self.n_folds])\n",
    "        train_books = set(all_books) - test_books - val_books\n",
    "        \n",
    "        train_samples, val_samples, test_samples = [], [], []\n",
    "        \n",
    "        for book_id, samples_list in by_book.items():\n",
    "            for i, s in samples_list:\n",
    "                converted = _convert_multi_source_sample(s, i, 'pdnc')\n",
    "                if converted is None:\n",
    "                    continue\n",
    "                if book_id in test_books:\n",
    "                    test_samples.append(converted)\n",
    "                elif book_id in val_books:\n",
    "                    val_samples.append(converted)\n",
    "                else:\n",
    "                    train_samples.append(converted)\n",
    "        \n",
    "        # Validate splits\n",
    "        if not train_samples or not val_samples or not test_samples:\n",
    "            raise RuntimeError(\n",
    "                f\"Fold {fold_idx} has empty splits: \"\n",
    "                f\"train={len(train_samples)}, val={len(val_samples)}, test={len(test_samples)}\"\n",
    "            )\n",
    "        \n",
    "        # Combine with other datasets if provided\n",
    "        if other_datasets:\n",
    "            print(f\"   + Adding datasets: {other_datasets}\")\n",
    "            other_train, other_val, other_test = load_datasets(self.base_path, other_datasets)\n",
    "            train_samples = train_samples + other_train\n",
    "            val_samples = val_samples + other_val\n",
    "            test_samples = test_samples + other_test\n",
    "        \n",
    "        return train_samples, val_samples, test_samples\n",
    "\n",
    "\n",
    "def load_datasets(base_path: str, datasets: list):\n",
    "    \"\"\"\n",
    "    Load datasets using MultiSourceDataLoader and convert to training format.\n",
    "    \"\"\"\n",
    "    if not datasets:\n",
    "        raise ValueError(\"datasets list is empty\")\n",
    "    \n",
    "    loader = MultiSourceDataLoader(base_path=base_path, datasets=datasets, seed=CONFIG.seed)\n",
    "    loader.load_all()\n",
    "    \n",
    "    # Use the proper split_by_genre method from the module\n",
    "    train_samples, val_samples, test_samples = loader.split_by_genre(\n",
    "        val_ratio=0.1,\n",
    "        test_ratio=0.1\n",
    "    )\n",
    "    \n",
    "    # Convert using unified helper function\n",
    "    train_converted = [_convert_multi_source_sample(s, i) for i, s in enumerate(train_samples)]\n",
    "    train_converted = [s for s in train_converted if s is not None]\n",
    "    \n",
    "    val_converted = [_convert_multi_source_sample(s, i) for i, s in enumerate(val_samples)]\n",
    "    val_converted = [s for s in val_converted if s is not None]\n",
    "    \n",
    "    test_converted = [_convert_multi_source_sample(s, i) for i, s in enumerate(test_samples)]\n",
    "    test_converted = [s for s in test_converted if s is not None]\n",
    "    \n",
    "    # Validate we got data\n",
    "    if not train_converted:\n",
    "        raise RuntimeError(f\"No training samples loaded from datasets: {datasets}\")\n",
    "    \n",
    "    return train_converted, val_converted, test_converted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PREPARE FOLD LOADING\n",
    "# =============================================================================\n",
    "\n",
    "datasets_to_load = CONFIG.datasets\n",
    "use_pdnc_folds = 'pdnc' in datasets_to_load\n",
    "other_datasets = [d for d in datasets_to_load if d != 'pdnc']\n",
    "\n",
    "if use_pdnc_folds:\n",
    "    fold_selection = CONFIG.fold_selection\n",
    "\n",
    "    if fold_selection == \"all\":\n",
    "        FOLDS_TO_TRAIN = list(range(5))\n",
    "    elif isinstance(fold_selection, list):\n",
    "        FOLDS_TO_TRAIN = fold_selection\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid fold_selection: {fold_selection}. Must be 'all' or list of ints.\")\n",
    "\n",
    "    print(f\"[NOTE] PDNC Folds to train: {FOLDS_TO_TRAIN}\")\n",
    "    \n",
    "    # Create lazy fold iterator\n",
    "    fold_iterator = PDNCFoldIterator(\n",
    "        base_path=CONFIG.multi_source_base,\n",
    "        n_folds=5,\n",
    "        seed=CONFIG.seed\n",
    "    )\n",
    "    print(f\"   [OK] Lazy fold iterator created (folds load on-demand)\")\n",
    "else:\n",
    "    FOLDS_TO_TRAIN = [0]  # Single iteration for multi-dataset\n",
    "    fold_iterator = None\n",
    "    print(f\"[NOTE] Training with datasets: {datasets_to_load}\")\n",
    "\n",
    "print(f\"[LOOP] Will train {len(FOLDS_TO_TRAIN)} fold(s): {FOLDS_TO_TRAIN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURSOR: No-op placeholder (do not delete the training repo mid-session)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING LOGGER AND CHECKPOINT MANAGEMENT\n",
    "# =============================================================================\n",
    "\n",
    "class TrainingLogger:\n",
    "    \"\"\"Logs training metrics to CSV with optional wandb integration.\"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir: str, run_name: str, config: dict = None):\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.csv_path = self.log_dir / \"training_log.csv\"\n",
    "        self.run_name = run_name\n",
    "        \n",
    "        # Initialize CSV\n",
    "        if not self.csv_path.exists():\n",
    "            with open(self.csv_path, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['timestamp', 'fold', 'epoch', 'step', 'metric', 'value'])\n",
    "        \n",
    "        # Initialize wandb if available (skip to avoid blocking)\n",
    "        self.wandb_run = None\n",
    "        # CURSOR: Disabled wandb by default to prevent hanging on login prompts\n",
    "        # Set WANDB_MODE=online and login first if you want wandb\n",
    "        if WANDB_AVAILABLE and config and os.environ.get(\"WANDB_MODE\") == \"online\":\n",
    "            try:\n",
    "                import signal\n",
    "                # CURSOR: Timeout wandb init to prevent indefinite hanging\n",
    "                self.wandb_run = wandb.init(\n",
    "                    project=\"quote-attribution-training\",\n",
    "                    name=run_name,\n",
    "                    config=config,\n",
    "                    reinit=True,\n",
    "                    settings=wandb.Settings(init_timeout=30)  # 30 second timeout\n",
    "                )\n",
    "                print(\"[OK] wandb logging enabled\")\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN]  wandb init failed (using CSV only): {e}\")\n",
    "        else:\n",
    "            print(\"[INFO]  wandb disabled (set WANDB_MODE=online to enable)\")\n",
    "    \n",
    "    def log(self, metrics: dict, fold: int = 0, epoch: int = 0, step: int = 0):\n",
    "        \"\"\"Log metrics to CSV and wandb.\"\"\"\n",
    "        timestamp = time.time()\n",
    "        \n",
    "        # Log to CSV\n",
    "        with open(self.csv_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for metric_name, value in metrics.items():\n",
    "                writer.writerow([timestamp, fold, epoch, step, metric_name, value])\n",
    "        \n",
    "        # Log to wandb\n",
    "        if self.wandb_run:\n",
    "            try:\n",
    "                wandb.log({**metrics, 'fold': fold, 'epoch': epoch, 'step': step})\n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "    def finish(self):\n",
    "        \"\"\"Close wandb run.\"\"\"\n",
    "        if self.wandb_run:\n",
    "            try:\n",
    "                wandb.finish()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "\n",
    "def find_latest_checkpoint(fold_output_dir: str) -> Optional[str]:\n",
    "    \"\"\"Find most recent checkpoint for fold.\"\"\"\n",
    "    fold_path = Path(fold_output_dir)\n",
    "    if not fold_path.exists():\n",
    "        return None\n",
    "    \n",
    "    checkpoints = list(fold_path.glob(\"checkpoint-*/\"))\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "    \n",
    "    # Sort by step number\n",
    "    checkpoints.sort(key=lambda p: int(p.name.split('-')[1]))\n",
    "    return str(checkpoints[-1])\n",
    "\n",
    "\n",
    "def cleanup_old_checkpoints(fold_output_dir: str, keep_last: int = 2):\n",
    "    \"\"\"\n",
    "    Smart checkpoint cleanup: keep last N checkpoints + best model.\n",
    "    Runs at fold boundaries to avoid training slowdown.\n",
    "    \"\"\"\n",
    "    fold_path = Path(fold_output_dir)\n",
    "    if not fold_path.exists():\n",
    "        return\n",
    "    \n",
    "    checkpoints = list(fold_path.glob(\"checkpoint-*/\"))\n",
    "    if len(checkpoints) <= keep_last:\n",
    "        return  # Nothing to clean\n",
    "    \n",
    "    # Sort by step number\n",
    "    checkpoints.sort(key=lambda p: int(p.name.split('-')[1]))\n",
    "    \n",
    "    # Keep only last N\n",
    "    to_delete = checkpoints[:-keep_last]\n",
    "    \n",
    "    for checkpoint_path in to_delete:\n",
    "        try:\n",
    "            import shutil\n",
    "            shutil.rmtree(checkpoint_path)\n",
    "            print(f\"   [CLEAN]  Cleaned old checkpoint: {checkpoint_path.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   [WARN]  Failed to delete {checkpoint_path}: {e}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING SETUP AND HELPERS\n",
    "# =============================================================================\n",
    "\n",
    "# CURSOR: Enable gradient checkpointing for all GPU configs - modern PyTorch/HF Trainer\n",
    "# CURSOR: handles this correctly with DDP. Required for DeBERTa-large on T4 GPUs.\n",
    "USE_GRADIENT_CHECKPOINTING = CONFIG.gradient_checkpointing\n",
    "print(f\"[CONFIG] Gradient checkpointing: {USE_GRADIENT_CHECKPOINTING}\")\n",
    "\n",
    "# Collate function for variable-length candidate masks\n",
    "def collate_fn(batch):\n",
    "    max_cands = max(len(item['candidate_masks']) for item in batch)\n",
    "    input_ids = torch.stack([b['input_ids'] for b in batch])\n",
    "    attention_mask = torch.stack([b['attention_mask'] for b in batch])\n",
    "    quote_mask = torch.stack([b['quote_mask'] for b in batch])\n",
    "    cand_masks, cand_attn = [], []\n",
    "    for b in batch:\n",
    "        masks = b['candidate_masks']\n",
    "        orig_len = len(masks) or 1\n",
    "        if not masks:\n",
    "            masks = [torch.zeros_like(b['input_ids'])]\n",
    "        pad_count = max_cands - orig_len\n",
    "        if pad_count > 0:\n",
    "            masks = masks + [torch.zeros_like(masks[0])] * pad_count\n",
    "        cand_masks.append(torch.stack(masks))\n",
    "        cand_attn.append(torch.tensor([1] * orig_len + [0] * pad_count, dtype=torch.long))\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'quote_mask': quote_mask,\n",
    "        'candidate_masks': torch.stack(cand_masks),\n",
    "        'candidate_attention_mask': torch.stack(cand_attn),\n",
    "        'labels': torch.stack([b['label_idx'] for b in batch]),\n",
    "    }\n",
    "\n",
    "# CURSOR: Custom callback for visible progress output in notebooks\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class ProgressPrintCallback(TrainerCallback):\n",
    "    \"\"\"Prints training progress to stdout for visibility in notebooks.\"\"\"\n",
    "    \n",
    "    def __init__(self, print_every=50):\n",
    "        self.print_every = print_every\n",
    "        self.last_print_step = 0\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step - self.last_print_step >= self.print_every:\n",
    "            loss = state.log_history[-1].get('loss', 0) if state.log_history else 0\n",
    "            lr = state.log_history[-1].get('learning_rate', 0) if state.log_history else 0\n",
    "            epoch = state.epoch or 0\n",
    "            total_steps = state.max_steps or 1\n",
    "            pct = (state.global_step / total_steps) * 100\n",
    "            print(f\"   [STATS] Step {state.global_step}/{total_steps} ({pct:.1f}%) | Epoch {epoch:.2f} | Loss: {loss:.4f} | LR: {lr:.2e}\", flush=True)\n",
    "            self.last_print_step = state.global_step\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        epoch = int(state.epoch) if state.epoch else 0\n",
    "        print(f\"   [LOOP] Epoch {epoch} complete\", flush=True)\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics:\n",
    "            acc = metrics.get('eval_accuracy', 0)\n",
    "            loss = metrics.get('eval_loss', 0)\n",
    "            print(f\"   [GRAPH] EVAL @ step {state.global_step}: accuracy={acc:.4f}, loss={loss:.4f}\", flush=True)\n",
    "\n",
    "\n",
    "# Custom Trainer for our model\n",
    "class QuoteAttributionTrainer(Trainer):\n",
    "    \"\"\"Custom trainer that handles our model's unique input format.\"\"\"\n",
    "    \n",
    "    def __init__(self, loss_fn=None, training_logger=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.custom_loss_fn = loss_fn\n",
    "        self.ce_loss = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        self.training_logger = training_logger\n",
    "\n",
    "    def _get_train_sampler(self, train_dataset: Optional[Dataset] = None):\n",
    "        train_dataset = train_dataset or self.train_dataset\n",
    "        if train_dataset is None or not hasattr(train_dataset, 'samples'):\n",
    "            return super()._get_train_sampler(train_dataset)\n",
    "\n",
    "        samples = train_dataset.samples\n",
    "\n",
    "        if CONFIG.use_curriculum:\n",
    "            classifier = DifficultyClassifier()\n",
    "            difficulty_indices = classifier.classify_dataset(samples)\n",
    "\n",
    "            sample_weights = None\n",
    "            if CONFIG.balance_genres:\n",
    "                from collections import Counter\n",
    "                genres = [s.get('genre', 'unknown') for s in samples]\n",
    "                counts = Counter(genres)\n",
    "                total = len(genres)\n",
    "                n_genres = max(1, len(counts))\n",
    "                sample_weights = [total / (n_genres * counts[g]) for g in genres]\n",
    "\n",
    "            # CURSOR: Curriculum sampler now gets epoch from trainer state\n",
    "            current_epoch = int(self.state.epoch) if self.state.epoch is not None else 0\n",
    "            return CurriculumSampler(\n",
    "                difficulty_indices=difficulty_indices,\n",
    "                config=CurriculumConfig(),\n",
    "                total_epochs=CONFIG.epochs,\n",
    "                current_epoch=current_epoch,\n",
    "                batch_size=self.args.per_device_train_batch_size,\n",
    "                seed=CONFIG.seed,\n",
    "                sample_weights=sample_weights,\n",
    "            )\n",
    "\n",
    "        if CONFIG.balance_genres:\n",
    "            from collections import Counter\n",
    "            from torch.utils.data import WeightedRandomSampler\n",
    "            genres = [s.get('genre', 'unknown') for s in samples]\n",
    "            counts = Counter(genres)\n",
    "            total = len(genres)\n",
    "            n_genres = max(1, len(counts))\n",
    "            weights = [total / (n_genres * counts[g]) for g in genres]\n",
    "            return WeightedRandomSampler(weights=weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "        return super()._get_train_sampler(train_dataset)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop('labels')\n",
    "\n",
    "        def forward_once():\n",
    "            logits, _ = model(\n",
    "                inputs['input_ids'],\n",
    "                inputs['attention_mask'],\n",
    "                inputs['quote_mask'],\n",
    "                inputs['candidate_masks'],\n",
    "                inputs['candidate_attention_mask']\n",
    "            )\n",
    "            if inputs['candidate_attention_mask'] is not None:\n",
    "                logits = logits.masked_fill(inputs['candidate_attention_mask'] == 0, -1e9)\n",
    "            return logits\n",
    "\n",
    "        logits1 = forward_once()\n",
    "        logits_out = logits1\n",
    "\n",
    "        # CURSOR: Enable R-Drop when configured by performing a second forward pass with dropout.\n",
    "        if self.custom_loss_fn is not None and getattr(self.custom_loss_fn, 'r_drop_alpha', 0) > 0:\n",
    "            logits2 = forward_once()\n",
    "            loss = self.custom_loss_fn(logits1, labels, logits2=logits2)\n",
    "            logits_out = (logits1 + logits2) / 2\n",
    "        elif self.custom_loss_fn is not None:\n",
    "            loss = self.custom_loss_fn(logits1, labels)\n",
    "        else:\n",
    "            loss = self.ce_loss(logits1, labels)\n",
    "\n",
    "        return (loss, {'logits': logits_out}) if return_outputs else loss\n",
    "\n",
    "# Metrics function\n",
    "def compute_metrics(eval_pred: EvalPrediction) -> Dict[str, float]:\n",
    "    logits, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    mask = labels >= 0\n",
    "    acc = (preds[mask] == labels[mask]).mean()\n",
    "    return {'accuracy': float(acc)}\n",
    "\n",
    "# Loss function - use CombinedLoss from module\n",
    "def get_loss_fn():\n",
    "    from losses.focal_loss import CombinedLoss\n",
    "    return CombinedLoss(\n",
    "        focal_gamma=CONFIG.focal_gamma,\n",
    "        label_smoothing=CONFIG.label_smoothing,\n",
    "        r_drop_alpha=CONFIG.r_drop_alpha,\n",
    "        use_r_drop=CONFIG.r_drop_alpha > 0,\n",
    "        ignore_index=-100,\n",
    "    )\n",
    "\n",
    "os.makedirs(CONFIG.output_dir, exist_ok=True)\n",
    "\n",
    "print(\"[OK] Training helpers ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MULTI-FOLD TRAINING LOOP\n",
    "# =============================================================================\n",
    "\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# CURSOR: Print immediately so user knows cell is running\n",
    "print(\"=\" * 70, flush=True)\n",
    "print(f\"[START] MULTI-FOLD TRAINING: {CONFIG.name}\", flush=True)\n",
    "print(\"   [WAIT] Initializing training logger...\", flush=True)\n",
    "\n",
    "# Initialize training logger (wandb init can be slow)\n",
    "training_logger = TrainingLogger(\n",
    "    log_dir=CONFIG.output_dir,\n",
    "    run_name=f\"target_{CONFIG.target_level}_{int(time.time())}\",\n",
    "    config={\n",
    "        'target_level': CONFIG.target_level,\n",
    "        'epochs': CONFIG.epochs,\n",
    "        'batch_size': CONFIG.batch_size,\n",
    "        'lr': CONFIG.lr,\n",
    "        'datasets': CONFIG.datasets,\n",
    "        'num_gpus': NUM_GPUS,\n",
    "    }\n",
    ")\n",
    "print(\"   [OK] Training logger ready\", flush=True)\n",
    "\n",
    "# Track results across all folds\n",
    "fold_results = {}\n",
    "all_fold_accuracies = []\n",
    "fold_timings = []\n",
    "print(f\"   Folds to train: {FOLDS_TO_TRAIN}\", flush=True)\n",
    "print(f\"   GPUs: {NUM_GPUS} | Batch/GPU: {CONFIG.batch_size}\", flush=True)\n",
    "print(f\"   Effective batch: {CONFIG.batch_size * NUM_GPUS * CONFIG.gradient_accumulation_steps}\", flush=True)\n",
    "print(\"=\" * 70, flush=True)\n",
    "\n",
    "for fold_idx in FOLDS_TO_TRAIN:\n",
    "    fold_start_time = time.time()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[DIR] FOLD {fold_idx + 1}/{len(FOLDS_TO_TRAIN)} (index={fold_idx})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Fold-specific output directory\n",
    "    fold_output_dir = f\"{CONFIG.output_dir}/fold_{fold_idx}\"\n",
    "    os.makedirs(fold_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Check if fold already completed (for resume support)\n",
    "    best_model_path = f\"{fold_output_dir}/best_model\"\n",
    "    model_file = Path(best_model_path) / \"pytorch_model.bin\"\n",
    "    model_safetensors = Path(best_model_path) / \"model.safetensors\"\n",
    "    \n",
    "    if Path(best_model_path).exists() and (model_file.exists() or model_safetensors.exists()):\n",
    "        # CURSOR: Validate model is actually loadable before skipping\n",
    "        try:\n",
    "            from transformers import AutoConfig\n",
    "            config_path = Path(best_model_path) / \"config.json\"\n",
    "            if config_path.exists():\n",
    "                AutoConfig.from_pretrained(best_model_path)\n",
    "                print(f\"[SKIP]  Fold {fold_idx} already trained and validated, skipping\")\n",
    "                print(f\"   Model exists at: {best_model_path}\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"[WARN]  Fold {fold_idx} model incomplete (no config.json), re-training...\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN]  Fold {fold_idx} model validation failed: {e}\")\n",
    "            print(f\"   Re-training fold {fold_idx}...\")\n",
    "    \n",
    "    # Load data for this fold (lazy loading)\n",
    "    if use_pdnc_folds:\n",
    "        print(f\"   [WAIT] Loading PDNC fold {fold_idx}...\", flush=True)\n",
    "        load_start = time.time()\n",
    "        train_samples, val_samples, test_samples = fold_iterator.load_fold(fold_idx, other_datasets)\n",
    "        print(f\"   [OK] Data loaded in {time.time() - load_start:.1f}s\", flush=True)\n",
    "    else:\n",
    "        print(f\"   [WAIT] Loading datasets: {datasets_to_load}...\", flush=True)\n",
    "        load_start = time.time()\n",
    "        train_samples, val_samples, test_samples = load_datasets(CONFIG.multi_source_base, datasets_to_load)\n",
    "        print(f\"   [OK] Data loaded in {time.time() - load_start:.1f}s\", flush=True)\n",
    "    \n",
    "    # CURSOR: Build realistic candidate sets and eliminate candidate-order label leakage.\n",
    "    print(f\"   [WAIT] Finalizing candidate sets...\", flush=True)\n",
    "    cand_start = time.time()\n",
    "    train_samples = finalize_candidate_sets(\n",
    "        train_samples,\n",
    "        seed=CONFIG.seed + fold_idx,\n",
    "        hard_negative_topk=CONFIG.hard_negative_topk,\n",
    "        max_candidates=CONFIG.max_candidates,\n",
    "        shuffle_candidates=CONFIG.shuffle_candidates,\n",
    "    )\n",
    "    print(f\"      Train candidates done ({len(train_samples)} samples)\", flush=True)\n",
    "    val_samples = finalize_candidate_sets(\n",
    "        val_samples,\n",
    "        seed=CONFIG.seed + fold_idx,\n",
    "        hard_negative_topk=CONFIG.hard_negative_topk,\n",
    "        max_candidates=CONFIG.max_candidates,\n",
    "        shuffle_candidates=CONFIG.shuffle_candidates,\n",
    "    )\n",
    "    print(f\"      Val candidates done ({len(val_samples)} samples)\", flush=True)\n",
    "    test_samples = finalize_candidate_sets(\n",
    "        test_samples,\n",
    "        seed=CONFIG.seed + fold_idx,\n",
    "        hard_negative_topk=CONFIG.hard_negative_topk,\n",
    "        max_candidates=CONFIG.max_candidates,\n",
    "        shuffle_candidates=CONFIG.shuffle_candidates,\n",
    "    )\n",
    "    print(f\"   [OK] Candidate sets finalized in {time.time() - cand_start:.1f}s\", flush=True)\n",
    "\n",
    "    # CURSOR: Quick leakage check (gold_index should not collapse to a constant).\n",
    "    gi_preview = [s.get('gold_index', -1) for s in train_samples[:2000]]\n",
    "    if gi_preview and len(set(gi_preview)) == 1:\n",
    "        print(f\"[WARN]  gold_index collapsed to {gi_preview[0]} for preview set; check candidate construction.\")\n",
    "\n",
    "    if not train_samples or not val_samples or not test_samples:\n",
    "        raise RuntimeError(\n",
    "            f\"Empty split after candidate finalization: train={len(train_samples)}, val={len(val_samples)}, test={len(test_samples)}\"\n",
    "        )\n",
    "\n",
    "    print(f\"   Train: {len(train_samples)} | Val: {len(val_samples)} | Test: {len(test_samples)}\")\n",
    "    \n",
    "    # Create fresh model for each fold (important for proper cross-validation)\n",
    "    print(f\"   [WAIT] Initializing fresh model ({CONFIG.base_model})...\", flush=True)\n",
    "    \n",
    "    model_start = time.time()\n",
    "    set_seed(CONFIG.seed + fold_idx)  # Different seed per fold\n",
    "    model = MaxPerformanceSpeakerModel(CONFIG.base_model)\n",
    "    if USE_GRADIENT_CHECKPOINTING:\n",
    "        model.encoder.gradient_checkpointing_enable()\n",
    "    tokenizer = model.get_tokenizer()\n",
    "    print(f\"   [OK] Model initialized in {time.time() - model_start:.1f}s\", flush=True)\n",
    "    \n",
    "    # Create augmenter if enabled\n",
    "    augmenter = QuoteAugmenter(seed=CONFIG.seed + fold_idx) if CONFIG.use_augmentation else None\n",
    "    \n",
    "    # CURSOR: Curriculum scheduling is handled by a sampler in the Trainer (no pre-sorting).\n",
    "    \n",
    "    # Create datasets\n",
    "    print(f\"   [WAIT] Creating datasets (tokenizing {len(train_samples)} train + {len(val_samples)} val samples)...\", flush=True)\n",
    "    ds_start = time.time()\n",
    "    train_dataset = QuoteDataset(\n",
    "        train_samples, tokenizer, CONFIG.max_length,\n",
    "        augment=CONFIG.use_augmentation, augmenter=augmenter\n",
    "    )\n",
    "    val_dataset = QuoteDataset(val_samples, tokenizer, CONFIG.max_length)\n",
    "    print(f\"   [OK] Datasets created in {time.time() - ds_start:.1f}s\", flush=True)\n",
    "    \n",
    "    # Check for resume checkpoint\n",
    "    resume_checkpoint = find_latest_checkpoint(fold_output_dir)\n",
    "    if resume_checkpoint:\n",
    "        print(f\"[LOOP] Resuming fold {fold_idx} from {resume_checkpoint}\")\n",
    "    \n",
    "    # CURSOR: Wrap model with DataParallel for multi-GPU (single process)\n",
    "    if USE_DATA_PARALLEL:\n",
    "        model = nn.DataParallel(model)\n",
    "        print(f\"   [OK] Model wrapped with DataParallel for {NUM_GPUS} GPUs\")\n",
    "    \n",
    "    # CURSOR: Set batch size - with DataParallel, batch is split across GPUs automatically\n",
    "    import math\n",
    "    if USE_DATA_PARALLEL:\n",
    "        # CURSOR: Total batch across all GPUs\n",
    "        effective_batch = CONFIG.batch_size \n",
    "        effective_grad_accum = CONFIG.gradient_accumulation_steps\n",
    "    else:\n",
    "        # CURSOR: Single GPU\n",
    "        if USE_GRADIENT_CHECKPOINTING:\n",
    "            effective_batch = min(CONFIG.batch_size, 4)\n",
    "        else:\n",
    "            effective_batch = 2\n",
    "        effective_batch = max(1, int(effective_batch))\n",
    "        effective_grad_accum = max(1, int(CONFIG.gradient_accumulation_steps * math.ceil(CONFIG.batch_size / effective_batch)))\n",
    "    \n",
    "    print(f\"   Batch total: {effective_batch} | Grad accum: {effective_grad_accum} | Effective: {effective_batch * effective_grad_accum}\")\n",
    "    \n",
    "    # Training arguments for this fold\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=fold_output_dir,\n",
    "        num_train_epochs=CONFIG.epochs,\n",
    "        per_device_train_batch_size=effective_batch // NUM_GPUS if USE_DATA_PARALLEL else effective_batch,\n",
    "        per_device_eval_batch_size=effective_batch // NUM_GPUS if USE_DATA_PARALLEL else effective_batch,\n",
    "        gradient_accumulation_steps=effective_grad_accum,\n",
    "        gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,  # CURSOR: Memory saving for DeBERTa-large\n",
    "        learning_rate=CONFIG.lr,\n",
    "        weight_decay=0.01,\n",
    "        fp16=CONFIG.fp16,\n",
    "        fp16_full_eval=CONFIG.fp16,  # CURSOR: Must match fp16 for DeepSpeed\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        \n",
    "        # Evaluation and saving\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=CONFIG.eval_every,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=CONFIG.checkpoint_every,\n",
    "        save_total_limit=3,  # Keep 3 checkpoints for safe resume\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        greater_is_better=True,\n",
    "        \n",
    "        # Logging - frequent updates for real-time monitoring\n",
    "        logging_steps=10,  # Log every 10 steps for real-time feedback\n",
    "        logging_first_step=True,\n",
    "        logging_strategy=\"steps\",\n",
    "        report_to=\"none\",  # Custom logging to CSV + optional wandb\n",
    "        disable_tqdm=False,  # Enable progress bars\n",
    "        \n",
    "        # Performance\n",
    "        dataloader_num_workers=0,\n",
    "        dataloader_pin_memory=True,\n",
    "        remove_unused_columns=False,\n",
    "        \n",
    "        # CURSOR: No DDP/DeepSpeed - using DataParallel instead\n",
    "        \n",
    "        # Seed\n",
    "        seed=CONFIG.seed + fold_idx,\n",
    "    )\n",
    "    \n",
    "    # Create trainer for this fold\n",
    "    trainer = QuoteAttributionTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=compute_metrics,\n",
    "        loss_fn=get_loss_fn(),\n",
    "        training_logger=training_logger,\n",
    "        callbacks=[ProgressPrintCallback(print_every=50)],  # CURSOR: Print progress every 50 steps\n",
    "    )\n",
    "    \n",
    "    # Train this fold\n",
    "    print(f\"\\n   [TRAIN] Training fold {fold_idx}...\", flush=True)\n",
    "    print(f\"   [GO]  GPU training starting NOW - you should see progress bars below!\", flush=True)\n",
    "    print(f\"   \" + \"=\"*50, flush=True)\n",
    "    train_result = trainer.train(resume_from_checkpoint=resume_checkpoint)\n",
    "    print(f\"   \" + \"=\"*50, flush=True)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(f\"\\n   [STATS] Evaluating fold {fold_idx}...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    fold_accuracy = eval_results.get('eval_accuracy', 0.0)\n",
    "    all_fold_accuracies.append(fold_accuracy)\n",
    "    \n",
    "    # Save best model for this fold\n",
    "    trainer.save_model(best_model_path)\n",
    "    print(f\"   [SAVE] Model saved: {best_model_path}\")\n",
    "    \n",
    "    # Detailed evaluation with modules\n",
    "    print(f\"\\n   [SEARCH] Running detailed evaluation...\")\n",
    "    \n",
    "    # 1. Confidence calibration\n",
    "    if CONFIG.calibrate_temperature:\n",
    "        try:\n",
    "            temp_scaler = TemperatureScaling()\n",
    "            # Get validation predictions for calibration\n",
    "            val_preds = trainer.predict(val_dataset)\n",
    "            val_logits = torch.from_numpy(val_preds.predictions)\n",
    "            val_labels = torch.from_numpy(val_preds.label_ids)\n",
    "            temp_scaler.calibrate(val_logits, val_labels)\n",
    "            optimal_temp = temp_scaler.get_temperature()\n",
    "            print(f\"   [MEASURE] Optimal temperature: {optimal_temp:.3f}\")\n",
    "            training_logger.log({'temperature': optimal_temp}, fold=fold_idx)\n",
    "        except Exception as e:\n",
    "            print(f\"   [WARN]  Temperature calibration failed: {e}\")\n",
    "    \n",
    "    # 2. Error analysis (sample N=1000 for speed)\n",
    "    try:\n",
    "        error_analyzer = ErrorAnalyzer()\n",
    "        analysis_samples = test_samples[:1000]\n",
    "        test_preds = trainer.predict(QuoteDataset(analysis_samples, tokenizer, CONFIG.max_length))\n",
    "        test_predictions = np.argmax(test_preds.predictions, axis=-1)\n",
    "        test_confidences = np.max(torch.softmax(torch.from_numpy(test_preds.predictions), dim=-1).numpy(), axis=-1)\n",
    "        \n",
    "        # CURSOR: Add errors one by one using the correct ErrorAnalyzer API\n",
    "        for i, (pred, label, sample, conf) in enumerate(zip(\n",
    "            test_predictions, test_preds.label_ids, analysis_samples, test_confidences\n",
    "        )):\n",
    "            if pred != label and label >= 0:\n",
    "                candidates = sample.get('candidates', [])\n",
    "                pred_speaker = candidates[pred] if 0 <= pred < len(candidates) else 'unknown'\n",
    "                actual_speaker = candidates[label] if 0 <= label < len(candidates) else 'unknown'\n",
    "                error_analyzer.add_error(\n",
    "                    sample_id=sample.get('quote_id', str(i)),\n",
    "                    text=sample.get('text', ''),\n",
    "                    quote=sample.get('text', '')[:200],\n",
    "                    predicted=pred_speaker,\n",
    "                    actual=actual_speaker,\n",
    "                    candidates=candidates,\n",
    "                    confidence=float(conf),\n",
    "                    genre=sample.get('genre', 'unknown')\n",
    "                )\n",
    "        \n",
    "        error_summary = error_analyzer.get_summary()\n",
    "        top_patterns = error_analyzer.get_top_error_patterns(n=3)\n",
    "        \n",
    "        print(f\"   [GRAPH] Error analysis (N={len(analysis_samples)}):\")\n",
    "        print(f\"      Total errors: {error_summary['total_errors']}\")\n",
    "        print(f\"      High confidence errors: {error_summary['high_confidence_errors']:.1%}\")\n",
    "        for error_type, count, pct in top_patterns:\n",
    "            print(f\"      {error_type.value}: {count} ({pct:.1%})\")\n",
    "        \n",
    "        training_logger.log({\n",
    "            'error_count': error_summary['total_errors'],\n",
    "            'high_confidence_error_rate': error_summary['high_confidence_errors'],\n",
    "        }, fold=fold_idx)\n",
    "    except Exception as e:\n",
    "        print(f\"   [WARN]  Error analysis failed: {e}\")\n",
    "    \n",
    "    # 3. Post-processing impact (if implemented)\n",
    "    try:\n",
    "        post_processor = PostProcessor()\n",
    "        post_processor.reset_context()\n",
    "        \n",
    "        # CURSOR: Build proper input format for PostProcessor.batch_process()\n",
    "        pp_samples = analysis_samples[:100]\n",
    "        pp_preds = test_predictions[:100]\n",
    "        pp_confs = test_confidences[:100]\n",
    "        pp_labels = test_preds.label_ids[:100]\n",
    "        \n",
    "        # Convert to format expected by PostProcessor\n",
    "        pp_input = []\n",
    "        for i, (sample, pred, conf) in enumerate(zip(pp_samples, pp_preds, pp_confs)):\n",
    "            candidates = sample.get('candidates', [])\n",
    "            pred_speaker = candidates[pred] if 0 <= pred < len(candidates) else 'unknown'\n",
    "            pp_input.append({\n",
    "                'speaker': pred_speaker,\n",
    "                'confidence': float(conf),\n",
    "                'quote': sample.get('text', '')[:200],\n",
    "                'context': sample.get('text', ''),\n",
    "                'candidates': candidates,\n",
    "                'position': i * 100,\n",
    "            })\n",
    "        \n",
    "        pp_results = post_processor.batch_process(pp_input)\n",
    "        \n",
    "        # Count improvements\n",
    "        n_improved = 0\n",
    "        for i, (result, label) in enumerate(zip(pp_results, pp_labels)):\n",
    "            if result.was_modified and label >= 0:\n",
    "                candidates = pp_samples[i].get('candidates', [])\n",
    "                actual_speaker = candidates[label] if 0 <= label < len(candidates) else ''\n",
    "                if result.speaker == actual_speaker:\n",
    "                    n_improved += 1\n",
    "        \n",
    "        print(f\"   [FIX] Post-processing improved: {n_improved}/100 samples\")\n",
    "        training_logger.log({'post_process_improvement': n_improved / 100}, fold=fold_idx)\n",
    "    except Exception as e:\n",
    "        print(f\"   [WARN]  Post-processing test failed: {e}\")\n",
    "    \n",
    "    # Log fold results\n",
    "    fold_elapsed = time.time() - fold_start_time\n",
    "    fold_timings.append(fold_elapsed)\n",
    "    \n",
    "    fold_results[fold_idx] = {\n",
    "        'accuracy': fold_accuracy,\n",
    "        'train_loss': train_result.training_loss,\n",
    "        'model_path': best_model_path,\n",
    "        'training_time': fold_elapsed,\n",
    "    }\n",
    "    \n",
    "    training_logger.log({\n",
    "        'fold_accuracy': fold_accuracy,\n",
    "        'fold_train_loss': train_result.training_loss,\n",
    "        'fold_time_seconds': fold_elapsed,\n",
    "    }, fold=fold_idx)\n",
    "    \n",
    "    print(f\"\\n   [OK] Fold {fold_idx} complete! ({fold_elapsed/60:.1f} minutes)\")\n",
    "    print(f\"      Accuracy: {fold_accuracy:.4f}\")\n",
    "    \n",
    "    # Clean up old checkpoints at fold boundary\n",
    "    cleanup_old_checkpoints(fold_output_dir, keep_last=2)\n",
    "    \n",
    "    # Clean up GPU memory before next fold\n",
    "    del model, trainer, train_dataset, val_dataset\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING SUMMARY AND VISUALIZATION\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"[WIN] MULTI-FOLD TRAINING COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\n[STATS] Results per fold:\")\n",
    "for fold_idx, results in fold_results.items():\n",
    "    print(f\"   Fold {fold_idx}: Accuracy = {results['accuracy']:.4f} ({results['training_time']/60:.1f} min)\")\n",
    "\n",
    "if len(all_fold_accuracies) > 1:\n",
    "    mean_acc = np.mean(all_fold_accuracies)\n",
    "    std_acc = np.std(all_fold_accuracies)\n",
    "    print(f\"\\n[GRAPH] Cross-validation summary:\")\n",
    "    print(f\"   Mean accuracy: {mean_acc:.4f}  {std_acc:.4f}\")\n",
    "    print(f\"   Min: {min(all_fold_accuracies):.4f} | Max: {max(all_fold_accuracies):.4f}\")\n",
    "    \n",
    "    training_logger.log({\n",
    "        'cv_mean_accuracy': mean_acc,\n",
    "        'cv_std_accuracy': std_acc,\n",
    "        'cv_min_accuracy': min(all_fold_accuracies),\n",
    "        'cv_max_accuracy': max(all_fold_accuracies),\n",
    "    })\n",
    "else:\n",
    "    print(f\"\\n[GRAPH] Single fold accuracy: {all_fold_accuracies[0]:.4f}\")\n",
    "\n",
    "print(f\"\\n[TIME]  Total training time: {sum(fold_timings)/3600:.2f} hours\")\n",
    "print(f\"[FILE] Models saved to: {CONFIG.output_dir}/fold_*/best_model\")\n",
    "\n",
    "# Generate training summary visualization\n",
    "print(f\"\\n[STATS] Generating training visualization...\")\n",
    "try:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Accuracy per fold\n",
    "    folds = sorted(fold_results.keys())\n",
    "    accs = [fold_results[f]['accuracy'] for f in folds]\n",
    "    axes[0, 0].bar(folds, accs, color='steelblue')\n",
    "    if len(accs) > 1:\n",
    "        axes[0, 0].axhline(np.mean(accs), color='red', linestyle='--', label=f'Mean: {np.mean(accs):.4f}')\n",
    "        axes[0, 0].legend()\n",
    "    axes[0, 0].set_xlabel('Fold')\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].set_title('Accuracy by Fold')\n",
    "    axes[0, 0].set_ylim([0.7, 1.0])\n",
    "    \n",
    "    # Plot 2: Training time per fold\n",
    "    times = [fold_results[f]['training_time']/60 for f in folds]\n",
    "    axes[0, 1].bar(folds, times, color='coral')\n",
    "    axes[0, 1].set_xlabel('Fold')\n",
    "    axes[0, 1].set_ylabel('Time (minutes)')\n",
    "    axes[0, 1].set_title('Training Time by Fold')\n",
    "    \n",
    "    # Plot 3: Read training log for loss curves\n",
    "    if training_logger.csv_path.exists():\n",
    "        log_df = pd.read_csv(training_logger.csv_path)\n",
    "        loss_df = log_df[log_df['metric'] == 'loss']\n",
    "        if not loss_df.empty:\n",
    "            for fold in folds:\n",
    "                fold_loss = loss_df[loss_df['fold'] == fold]\n",
    "                if not fold_loss.empty:\n",
    "                    axes[1, 0].plot(fold_loss['step'], fold_loss['value'], label=f'Fold {fold}', alpha=0.7)\n",
    "            axes[1, 0].set_xlabel('Step')\n",
    "            axes[1, 0].set_ylabel('Loss')\n",
    "            axes[1, 0].set_title('Training Loss Curves')\n",
    "            axes[1, 0].legend()\n",
    "    \n",
    "    # Plot 4: Summary statistics\n",
    "    stats_text = f\"\"\"\n",
    "Training Summary\n",
    "{'='*30}\n",
    "Target Level: {CONFIG.target_level}\n",
    "Datasets: {', '.join(CONFIG.datasets)}\n",
    "Folds Trained: {len(folds)}\n",
    "\n",
    "Cross-Validation:\n",
    "  Mean Accuracy: {np.mean(accs):.4f}\n",
    "  Std Accuracy: {np.std(accs):.4f}\n",
    "  Min/Max: {min(accs):.4f} / {max(accs):.4f}\n",
    "\n",
    "Training Time:\n",
    "  Total: {sum(fold_timings)/3600:.2f} hours\n",
    "  Avg/Fold: {np.mean(fold_timings)/60:.1f} min\n",
    "\"\"\"\n",
    "    axes[1, 1].text(0.1, 0.5, stats_text, fontsize=10, family='monospace', \n",
    "                    verticalalignment='center')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = f\"{CONFIG.output_dir}/training_summary.png\"\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"   [OK] Saved visualization: {plot_path}\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"   [WARN]  Visualization failed: {e}\")\n",
    "\n",
    "# Close training logger\n",
    "training_logger.finish()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "\n",
    "print(\"[OK] Training complete! Models ready for inference.\")\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
